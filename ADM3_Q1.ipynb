{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from lxml import html\n",
    "import requests\n",
    "from langdetect import detect\n",
    "import math\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be changed to 201 to 301\n",
    "outfile = open(\"test.txt\", \"w\")\n",
    "for i in range(201,301):\n",
    "    \n",
    "    base_url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(i)\n",
    "\n",
    "    html = urlopen(base_url)\n",
    "    soup = BeautifulSoup(html.read(), features=\"lxml\")\n",
    "\n",
    "    links = []\n",
    "    regex = \"/book/show/.\"\n",
    "\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        if re.match(regex, str(link.get(\"href\"))):\n",
    "            links.append(\"https://www.goodreads.com\"+link.get(\"href\"))\n",
    "    \n",
    "    #issues of duplicates        \n",
    "    links = list(dict.fromkeys(links))\n",
    "    \n",
    "    for link in links:\n",
    "        outfile.write(link+\"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTMLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I make sure i am in the directory that has the text file with the URLS\n",
    "path = \"C:/Users/Guillaume/Desktop/ADM_HM3\"\n",
    "os.chdir(path)\n",
    "\n",
    "i = 1\n",
    "page = 1\n",
    "\n",
    "with open(\"test_urls.txt\", \"r\") as urls:\n",
    "    # I previously created the 100 directories, from page 201 to page 300\n",
    "    # I change the working directory for which i want the HTMLs \n",
    "    path = \"C:/Users/Guillaume/Desktop/ADM_HM3/page_201\"\n",
    "    os.chdir(path)\n",
    "\n",
    "    for url in urls.readlines()[0:]:\n",
    "\n",
    "        html = urlopen(url)\n",
    "\n",
    "        soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "        \n",
    "        # The logig here is not the best but it works- \n",
    "        # Basically, once every 100 urls treated I chance the directory\n",
    "        \n",
    "        if (i%101) != 0:\n",
    "                # We stay in the current directory\n",
    "            with open(\"article_\"+str(i)+\".html\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "                outfile.write(str(soup))\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        else :\n",
    "\n",
    "            page = page + 1\n",
    "            path = \"C:/Users/Guillaume/Desktop/ADM_HM3/page_20\"+str(page)\n",
    "            os.chdir(path)\n",
    "\n",
    "            with open(\"article_\"+str(i)+\".html\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "                outfile.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsingin HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All functions to retrieve the informations\n",
    "\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find(id=\"bookTitle\").contents[0].strip()\n",
    "        return title\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def get_series(soup):\n",
    "    try:\n",
    "        serie = soup.find(id=\"bookSeries\").get_text().strip().replace(\"(\",\"\").replace(\")\",\"\")\n",
    "        regex = \"#.*\"\n",
    "        serie = re.sub(regex, \"\",serie)\n",
    "        return serie\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def get_author(soup):\n",
    "    try:\n",
    "        author = soup.find_all(class_=\"authorName__container\")\n",
    "        author_list = \" \".join([x.get_text().strip() for x in author])\n",
    "        return author_list\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def get_pages(soup):\n",
    "    try:\n",
    "        pages = soup.find(itemprop =\"numberOfPages\").get_text().split()[0]\n",
    "        return int(pages)\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def get_ratingValue(soup):\n",
    "    try:\n",
    "        ratingValue = soup.find(itemprop =\"ratingValue\").get_text().split()[0]\n",
    "        return float(ratingValue)\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def get_ratingCount(soup):\n",
    "    try:\n",
    "        test = []\n",
    "        ratingCount = soup.find_all(class_ =\"gr-hyperlink\", href=\"#other_reviews\")\n",
    "        for x in ratingCount:\n",
    "            test.append(x.get_text().split())\n",
    "        return int(test[0][0].replace(\",\",\"\"))\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def get_reviewCount(soup):\n",
    "    try:\n",
    "        test = []\n",
    "        ratingCount = soup.find_all(class_ =\"gr-hyperlink\", href=\"#other_reviews\")\n",
    "        for x in ratingCount:\n",
    "            test.append(x.get_text().split())\n",
    "        return int(test[1][0].replace(\",\",\"\"))\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def get_date(soup):\n",
    "    try:\n",
    "        regex = \"by.*\"\n",
    "        date = soup.find_all(class_ = \"row\")\n",
    "        date = re.sub(regex,\"\" ,str(date[1].contents[0]).replace(\"Published\",\"\")).strip()\n",
    "        return date\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def get_setting(soup):\n",
    "    try:\n",
    "        string = re.compile(\"/places/.\")\n",
    "        test = soup.find_all(href=string)\n",
    "        places_list = \" \".join([(str(a.contents[0])) for a in test])\n",
    "        return places_list\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def get_plot(soup):\n",
    "    try:\n",
    "        plot = soup.find(id=\"description\")\n",
    "        plot = plot.find_all(id= re.compile(\"freeText\\.*\"))\n",
    "\n",
    "        final_plot = [] # It seems i need a list because joining on an empty string gives me a !\"Â£$%&\"@ blank\n",
    "\n",
    "    # now there should not be a case where there are 3 span tags ...hopefully\n",
    "        if len(plot) > 1:\n",
    "            final_plot.append(plot[1].get_text())\n",
    "        else:\n",
    "            final_plot.append(plot[0].get_text())\n",
    "\n",
    "        return final_plot[0]\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_char(soup):\n",
    "    try:\n",
    "        string = re.compile(\"/characters/.\")\n",
    "        test = soup.find_all(href=string)\n",
    "\n",
    "        char_list= \" \".join([(str(a.contents[0])) for a in test])\n",
    "\n",
    "        return char_list\n",
    "    except AttributeError:\n",
    "        return \"\"\n",
    "\n",
    "def get_url(soup):\n",
    "    try:\n",
    "        regex = \"https://www.goodreads.com/book/show/.\"\n",
    "        ratingCount = soup.find(href =re.compile(regex), rel=\"canonical\")\n",
    "        return ratingCount.get('href')\n",
    "    except AttributeError:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_count = 0\n",
    "nb_html_articles = 100\n",
    "page = 1\n",
    "article_nb = 1\n",
    "\n",
    "for k in range(1,100):\n",
    "\n",
    "    path = \"C:/Users/Guillaume/Desktop/ADM_HM3/page_\"+str(page)\n",
    "    os.chdir(path)\n",
    "\n",
    "    for i in range(1,nb_html_articles+1):\n",
    "\n",
    "        path = \"C:/Users/Guillaume/Desktop/ADM_HM3/page_\"+str(page)\n",
    "        os.chdir(path)\n",
    "        html = open(\"article_\"+str(article_nb)+\".html\", \"r\", encoding=\"utf-8\")\n",
    "        soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "        article_nb += 1\n",
    "\n",
    "        try:\n",
    "            if detect(get_plot(soup)) == \"en\":\n",
    "                eng_count += 1\n",
    "\n",
    "                path = \"C:/Users/Guillaume/Desktop/ADM_HM3/TSVs\"\n",
    "                os.chdir(path)\n",
    "                  \n",
    "                with open(\"article_\"+str(eng_count)+\".tsv\", 'w', encoding=\"utf-8\") as f_output:\n",
    "                    test_list_tsv = [get_title(soup), \n",
    "                    get_series(soup), \n",
    "                    get_author(soup),\n",
    "                    get_ratingValue(soup),\n",
    "                    get_ratingCount(soup), \n",
    "                    get_reviewCount(soup),\n",
    "                    get_plot(soup),\n",
    "                    get_pages(soup),\n",
    "                    get_date(soup),\n",
    "                    get_setting(soup),\n",
    "                    get_char(soup),\n",
    "                    get_url(soup)]\n",
    "\n",
    "                    header_list = [\"bookTitle\",\"bookSeries\",\"bookAuthors\",\"ratingValue\",\"ratingCount\",\"reviewCount\",\"Plot\",\"NumberofPages\",\"Date\",\"Characters\",\"Setting\",\"URL\"]\n",
    "\n",
    "                    tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "                    tsv_output.writerow(header_list)\n",
    "                    tsv_output.writerow(test_list_tsv)\n",
    "        except:\n",
    "            language =\"error\"\n",
    "        #ErrorCode.CantDetectError\n",
    "    page = page + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Guillaume/Desktop/ADM_HM3/\n",
    "os.chdir(path)\n",
    "\n",
    "df = pd.DataFrame(columns = [\"bookTitle\",\"Plot\",\"URL\",\"Characters\",\"ratingValue\",\"Date\"])\n",
    "\n",
    "nb_articles = 26885\n",
    "\n",
    "for i in range(1,nb_articles+1):\n",
    "    article = open(\"article_\"+str(i)+\".tsv\", \"r\", encoding=\"utf-8\")\n",
    "    data = pd.read_csv(article, sep = \"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "    title = data[\"bookTitle\"].iloc[0]\n",
    "    plot = data[\"Plot\"].iloc[0]\n",
    "    url = data[\"URL\"].iloc[0]\n",
    "    characters = data[\"Setting\"].iloc[0]\n",
    "    ratingValue = data[\"ratingValue\"].iloc[0]\n",
    "    date = data[\"Date\"].iloc[0]\n",
    "\n",
    "    to_append = [title, plot, url,characters,ratingValue,date]\n",
    "    df_length = len(df)\n",
    "    df.loc[df_length] = to_append\n",
    "\n",
    "# change to CVS file name you wish\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
