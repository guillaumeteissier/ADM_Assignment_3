{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from lxml import html\n",
    "import requests\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.1 -> Getting all the URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be changed to 201 to 301\n",
    "outfile = open(\"test.txt\", \"w\")\n",
    "for i in range(201,301):\n",
    "    \n",
    "    base_url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(i)\n",
    "\n",
    "    html = urlopen(base_url)\n",
    "    soup = BeautifulSoup(html.read(), features=\"lxml\")\n",
    "\n",
    "    links = []\n",
    "    regex = \"/book/show/.\"\n",
    "\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        if re.match(regex, str(link.get(\"href\"))):\n",
    "            links.append(\"https://www.goodreads.com\"+link.get(\"href\"))\n",
    "    \n",
    "    #issues of duplicates        \n",
    "    links = list(dict.fromkeys(links))\n",
    "    \n",
    "    for link in links:\n",
    "        outfile.write(link+\"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.2 -> getting the HTML from the URLs, getting these HTMLs in different files\n",
    "\n",
    "do not run the code below, this is just to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "# First, I make sure i am in the directory that has the text file with the URLS\n",
    "path = \"C:/Users/Guillaume/Desktop/ADM_HM3\"\n",
    "os.chdir(path)\n",
    "\n",
    "i = 1\n",
    "page = 1\n",
    "\n",
    "with open(\"test_urls.txt\", \"r\") as urls:\n",
    "    # I previously created the 100 directories, from page 201 to page 300\n",
    "    # I change the working directory for which i want the HTMLs \n",
    "    path = \"C:/Users/Guillaume/Desktop/ADM_HM3/page_201\"\n",
    "    os.chdir(path)\n",
    "\n",
    "    for url in urls.readlines():\n",
    "\n",
    "        html = urlopen(url)\n",
    "\n",
    "        soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "        \n",
    "        # The logig here is not the best but it works- \n",
    "        # Basically, once every 100 urls treated I chance the directory\n",
    "        \n",
    "        if (i%101) != 0:\n",
    "                # We stay in the current directory\n",
    "            with open(\"article_\"+str(i)+\".html\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "                outfile.write(str(soup))\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        else :\n",
    "\n",
    "            page = page + 1\n",
    "            path = \"C:/Users/Guillaume/Desktop/ADM_HM3/page_20\"+str(page)\n",
    "            os.chdir(path)\n",
    "\n",
    "            with open(\"article_\"+str(i)+\".html\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "                outfile.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has been done locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.3 -> Parsing the HTMLs files to retrieve all the required info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All functions to retrieve the informations\n",
    "\n",
    "def get_title(soup):\n",
    "    title = soup.find(id=\"bookTitle\").contents[0].strip()\n",
    "    return title\n",
    "\n",
    "def get_series(soup):\n",
    "    serie = soup.find(id=\"bookSeries\").get_text().strip()\n",
    "    return serie\n",
    "\n",
    "def get_author(soup):\n",
    "    author = soup.find_all(class_=\"authorName__container\")\n",
    "    author_list = [x.get_text().strip() for x in author]\n",
    "    return author_list\n",
    "\n",
    "def get_pages(soup):\n",
    "    pages = soup.find(itemprop =\"numberOfPages\").get_text().split()[0]\n",
    "    return int(pages)\n",
    "\n",
    "def get_ratingValue(soup):\n",
    "    ratingValue = soup.find(itemprop =\"ratingValue\").get_text().split()[0]\n",
    "    return float(ratingValue)\n",
    "\n",
    "def get_ratingCount(soup):\n",
    "    ratingCount = soup.find(itemprop =\"ratingCount\").get_text().split()[0].replace(\",\",\"\")\n",
    "    return (ratingCount)\n",
    "\n",
    "def get_reviewCount(soup):\n",
    "    reviewCount = soup.find(itemprop =\"reviewCount\").get_text().split()[0].replace(\",\",\"\")\n",
    "    return (reviewCount)\n",
    "\n",
    "def get_date(soup):\n",
    "    regex = \"by.*\"\n",
    "    date = soup.find_all(class_ = \"row\")\n",
    "    date = re.sub(regex,\"\" ,str(date[1].contents[0]).replace(\"Published\",\"\")).strip()\n",
    "    return date\n",
    "\n",
    "def get_setting(url):\n",
    "    string = re.compile(\"/places/.\")\n",
    "    test = soup.find_all(href=string)\n",
    "    \n",
    "    char_list= []\n",
    "    for a in test:\n",
    "        char_list.append(str(a.contents[0]))\n",
    "\n",
    "    return char_list\n",
    "\n",
    "def get_plot(soup):\n",
    "    plot = soup.find(id=\"description\").get_text().strip()\n",
    "    return plot \n",
    "\n",
    "def get_char(soup):\n",
    "    string = re.compile(\"/characters/.\")\n",
    "    test = soup.find_all(href=string)\n",
    "\n",
    "    char_list= []\n",
    "    for a in test:\n",
    "        char_list.append(str(a.contents[0]))\n",
    "\n",
    "    return char_list\n",
    "\n",
    "def get_url(soup):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3722b64cac0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                              \u001b[0mget_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                              \u001b[0mget_ratingValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                              \u001b[0mget_ratingCount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                              \u001b[0mget_reviewCount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                              \u001b[0mget_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-1ef713061224>\u001b[0m in \u001b[0;36mget_ratingCount\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_ratingCount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mratingCount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemprop\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"ratingCount\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mratingCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# We want 1 TSV file for 1 article\n",
    "# In order to do this -> we first loop over the HTML\n",
    "# We check if the language is indeed english\n",
    "# for each HTML we create a new TSV and input the required DATA\n",
    "\n",
    "i = 1\n",
    "\n",
    "while i <= 2:\n",
    "    html = open(\"article_\"+str(i)+\".html\", \"r\", encoding=\"utf-8\")\n",
    "    soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "        \n",
    "    with open(\"html_tsv_test_\"+str(i)+\".tsv\", 'w', encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        # We check the language of the splot\n",
    "        if detect(get_plot(soup)) == \"en\":\n",
    "            \n",
    "            test_list_tsv = [get_title(soup), \n",
    "                             get_series(soup), \n",
    "                             get_author(soup),\n",
    "                             get_pages(soup),\n",
    "                             get_ratingValue(soup), \n",
    "                             #get_ratingCount(soup), \n",
    "                             #get_reviewCount(soup), \n",
    "                             get_date(soup),\n",
    "                             get_setting(soup),\n",
    "                             get_char(soup), \n",
    "                             get_plot(soup)]\n",
    "\n",
    "            tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "            tsv_output.writerow(test_list_tsv)\n",
    "            \n",
    "            i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2527738'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ratingCount(soup):\n",
    "    ratingCount = soup.find(itemprop =\"ratingCount\").get_text().split()[0].replace(\",\",\"\")\n",
    "    return (ratingCount)\n",
    "\n",
    "html = open(\"article_2.html\", \"r\", encoding=\"utf-8\")\n",
    "soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "get_ratingCount(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative with xpath -> did not find a way to go from soup to tree -> and I don't want to have 2 ways in parallel\n",
    "    bookTitle = tree.xpath(\"//h1[@id='bookTitle']/text()\")[0].strip()\n",
    "        bookSeries = tree.xpath('//*[@id=\"bookSeries\"]/a/text()')[0].strip()\n",
    "        bookAuthors = tree.xpath(\"//span[@itemprop='name']/text()\")[0].strip()\n",
    "        NumberofPages = int(tree.xpath(\"//span [@itemprop='numberOfPages']/text()\")[0].split()[0])\n",
    "        ratingValue = float(tree.xpath(\"//span [@itemprop='ratingValue']/text()\")[0].strip())\n",
    "        ratingCount = int((tree.xpath('//*[@id=\"bookMeta\"]/a[2]/text()')[1].split()[0]).replace(\",\",\"\"))\n",
    "        reviewCount = int((tree.xpath(\"//*[@id='bookMeta']/a[3]/text()\")[1].split()[0]).replace(\",\",\"\"))\n",
    "        PublishingDate = tree.xpath('//*[@id=\"details\"]/div[2]/text()')[0].split()[1:4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
