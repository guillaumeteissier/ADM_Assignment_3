{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from lxml import html\n",
    "import requests\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.1 -> Getting all the URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be changed to 201 to 301\n",
    "outfile = open(\"test.txt\", \"w\")\n",
    "for i in range(201,301):\n",
    "    \n",
    "    base_url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(i)\n",
    "\n",
    "    html = urlopen(base_url)\n",
    "    soup = BeautifulSoup(html.read(), features=\"lxml\")\n",
    "\n",
    "    links = []\n",
    "    regex = \"/book/show/.\"\n",
    "\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        if re.match(regex, str(link.get(\"href\"))):\n",
    "            links.append(\"https://www.goodreads.com\"+link.get(\"href\"))\n",
    "    \n",
    "    #issues of duplicates        \n",
    "    links = list(dict.fromkeys(links))\n",
    "    \n",
    "    for link in links:\n",
    "        outfile.write(link+\"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.2 -> getting the HTML from the URLs, getting these HTMLs in different files\n",
    "\n",
    "do not run the code below, this is just to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "# First, I make sure i am in the directory that has the text file with the URLS\n",
    "path = \"C:/Users/Guillaume/Desktop/ADM_HM3\"\n",
    "os.chdir(path)\n",
    "\n",
    "i = 1\n",
    "page = 1\n",
    "\n",
    "with open(\"test_urls.txt\", \"r\") as urls:\n",
    "    # I previously created the 100 directories, from page 201 to page 300\n",
    "    # I change the working directory for which i want the HTMLs \n",
    "    path = \"C:/Users/Guillaume/Desktop/ADM_HM3/page_201\"\n",
    "    os.chdir(path)\n",
    "\n",
    "    for url in urls.readlines()[0:]:\n",
    "\n",
    "        html = urlopen(url)\n",
    "\n",
    "        soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "        \n",
    "        # The logig here is not the best but it works- \n",
    "        # Basically, once every 100 urls treated I chance the directory\n",
    "        \n",
    "        if (i%101) != 0:\n",
    "                # We stay in the current directory\n",
    "            with open(\"article_\"+str(i)+\".html\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "                outfile.write(str(soup))\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        else :\n",
    "\n",
    "            page = page + 1\n",
    "            path = \"C:/Users/Guillaume/Desktop/ADM_HM3/page_20\"+str(page)\n",
    "            os.chdir(path)\n",
    "\n",
    "            with open(\"article_\"+str(i)+\".html\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "                outfile.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has been done locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.3 -> Parsing the HTMLs files to retrieve all the required info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All functions to retrieve the informations\n",
    "\n",
    "def get_title(soup):\n",
    "    title = soup.find(id=\"bookTitle\").contents[0].strip()\n",
    "    return title\n",
    "\n",
    "def get_series(soup):\n",
    "    serie = soup.find(id=\"bookSeries\").get_text().strip().replace(\"(\",\"\").replace(\")\",\"\")\n",
    "    regex = \"#.*\"\n",
    "    serie = re.sub(regex, \"\",serie)\n",
    "    return serie\n",
    "\n",
    "def get_author(soup):\n",
    "    author = soup.find_all(class_=\"authorName__container\")\n",
    "    author_list = \" \".join([x.get_text().strip() for x in author])\n",
    "    return author_list\n",
    "\n",
    "def get_pages(soup):\n",
    "    pages = soup.find(itemprop =\"numberOfPages\").get_text().split()[0]\n",
    "    return int(pages)\n",
    "\n",
    "def get_ratingValue(soup):\n",
    "    ratingValue = soup.find(itemprop =\"ratingValue\").get_text().split()[0]\n",
    "    return float(ratingValue)\n",
    "\n",
    "def get_ratingCount(soup):\n",
    "    test = []\n",
    "    ratingCount = soup.find_all(class_ =\"gr-hyperlink\", href=\"#other_reviews\")\n",
    "    for x in ratingCount:\n",
    "        test.append(x.get_text().split())\n",
    "    return int(test[0][0].replace(\",\",\"\"))\n",
    "    \n",
    "def get_reviewCount(soup):\n",
    "    test = []\n",
    "    ratingCount = soup.find_all(class_ =\"gr-hyperlink\", href=\"#other_reviews\")\n",
    "    for x in ratingCount:\n",
    "        test.append(x.get_text().split())\n",
    "    return int(test[1][0].replace(\",\",\"\"))\n",
    "\n",
    "def get_date(soup):\n",
    "    regex = \"by.*\"\n",
    "    date = soup.find_all(class_ = \"row\")\n",
    "    date = re.sub(regex,\"\" ,str(date[1].contents[0]).replace(\"Published\",\"\")).strip()\n",
    "    return date\n",
    "\n",
    "def get_setting(soup):\n",
    "    string = re.compile(\"/places/.\")\n",
    "    test = soup.find_all(href=string)\n",
    "    \n",
    "    places_list = \" \".join([(str(a.contents[0])) for a in test])\n",
    "\n",
    "    return places_list\n",
    "\n",
    "def get_plot(soup):\n",
    "    plot = soup.find(id=\"description\").get_text().strip()\n",
    "    return plot \n",
    "\n",
    "def get_char(soup):\n",
    "    string = re.compile(\"/characters/.\")\n",
    "    test = soup.find_all(href=string)\n",
    "\n",
    "    char_list= \" \".join([(str(a.contents[0])) for a in test])\n",
    "\n",
    "    return char_list\n",
    "\n",
    "def get_url(soup):\n",
    "\n",
    "    regex = \"https://www.goodreads.com/book/show/.\"\n",
    "    ratingCount = soup.find(href =re.compile(regex), rel=\"canonical\")\n",
    "    return ratingCount.get('href')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want 1 TSV file for 1 article\n",
    "# In order to do this -> we first loop over the HTML\n",
    "# We check if the language is indeed english\n",
    "# for each HTML we create a new TSV and input the required DATA\n",
    "\n",
    "i = 4\n",
    "\n",
    "while i <= 4:\n",
    "    html = open(\"article_\"+str(i)+\".html\", \"r\", encoding=\"utf-8\")\n",
    "    soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "        \n",
    "    with open(\"html_tsv_test_\"+str(i)+\".tsv\", 'w', encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        # We check the language of the splot\n",
    "        if detect(get_plot(soup)) == \"en\":\n",
    "            \n",
    "            test_list_tsv = [get_title(soup), \n",
    "                             get_series(soup), \n",
    "                             get_author(soup),\n",
    "                             get_ratingValue(soup),\n",
    "                             get_ratingCount(soup), \n",
    "                             get_reviewCount(soup),\n",
    "                             get_plot(soup),\n",
    "                             get_pages(soup),\n",
    "                             get_date(soup),\n",
    "                             get_setting(soup),\n",
    "                             get_char(soup),\n",
    "                             get_url(soup)]\n",
    "            \n",
    "            header_list = [\"bookTitle\",\"bookSeries\",\"bookAuthors\",\"ratingValue\",\"ratingCount\",\"reviewCount\",\"Plot\",\"NumberofPages\",\"Date\",\"Characters\",\"Setting\",\"URL\"]\n",
    "\n",
    "            tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "            tsv_output.writerow(header_list)\n",
    "            tsv_output.writerow(test_list_tsv)\n",
    "            \n",
    "            i = i + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "- Removing stopwords\n",
    "- Removing punctuation\n",
    "- Stemming\n",
    "- Anything else you think it's needed ?\n",
    "\n",
    "What is \"anything else\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'power', 'novel', 'spiritu', 'bond', 'man', 'anim'],\n",
       " ['there',\n",
       "  'door',\n",
       "  'end',\n",
       "  'silent',\n",
       "  'corridor',\n",
       "  'and',\n",
       "  'haunt',\n",
       "  'harri',\n",
       "  'pottter',\n",
       "  'dream',\n",
       "  'whi',\n",
       "  'els',\n",
       "  'would',\n",
       "  'wake',\n",
       "  'middl',\n",
       "  'night',\n",
       "  'scream',\n",
       "  'terror',\n",
       "  'harri',\n",
       "  'lot',\n",
       "  'mind',\n",
       "  'fifth',\n",
       "  'year',\n",
       "  'hogwart',\n",
       "  'defens',\n",
       "  'against',\n",
       "  'dark',\n",
       "  'art',\n",
       "  'teacher',\n",
       "  'person',\n",
       "  'like',\n",
       "  'poison',\n",
       "  'honey',\n",
       "  'big',\n",
       "  'surpris',\n",
       "  'gryffindor',\n",
       "  'quidditch',\n",
       "  'team',\n",
       "  'loomi',\n",
       "  'there',\n",
       "  'door',\n",
       "  'end',\n",
       "  'silent',\n",
       "  'corridor',\n",
       "  'and',\n",
       "  'haunt',\n",
       "  'harri',\n",
       "  'pottter',\n",
       "  'dream',\n",
       "  'whi',\n",
       "  'els',\n",
       "  'would',\n",
       "  'wake',\n",
       "  'middl',\n",
       "  'night',\n",
       "  'scream',\n",
       "  'terror',\n",
       "  'harri',\n",
       "  'lot',\n",
       "  'mind',\n",
       "  'fifth',\n",
       "  'year',\n",
       "  'hogwart',\n",
       "  'defens',\n",
       "  'against',\n",
       "  'dark',\n",
       "  'art',\n",
       "  'teacher',\n",
       "  'person',\n",
       "  'like',\n",
       "  'poison',\n",
       "  'honey',\n",
       "  'big',\n",
       "  'surpris',\n",
       "  'gryffindor',\n",
       "  'quidditch',\n",
       "  'team',\n",
       "  'loom',\n",
       "  'terror',\n",
       "  'ordinari',\n",
       "  'wizard',\n",
       "  'level',\n",
       "  'exam',\n",
       "  'but',\n",
       "  'thing',\n",
       "  'pale',\n",
       "  'next',\n",
       "  'grow',\n",
       "  'threat',\n",
       "  'threat',\n",
       "  'neither',\n",
       "  'magic',\n",
       "  'govern',\n",
       "  'author',\n",
       "  'hogwart',\n",
       "  'grasp',\n",
       "  'dark',\n",
       "  'tighten',\n",
       "  'harri',\n",
       "  'must',\n",
       "  'discov',\n",
       "  'true',\n",
       "  'depth',\n",
       "  'strength',\n",
       "  'friend',\n",
       "  'import',\n",
       "  'boundless',\n",
       "  'loyalti',\n",
       "  'shock',\n",
       "  'price',\n",
       "  'unbear',\n",
       "  'fate',\n",
       "  'depend'],\n",
       " ['could',\n",
       "  'surviv',\n",
       "  'wild',\n",
       "  'everi',\n",
       "  'one',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'live',\n",
       "  'see',\n",
       "  'morn',\n",
       "  'In',\n",
       "  'ruin',\n",
       "  'place',\n",
       "  'known',\n",
       "  'north',\n",
       "  'america',\n",
       "  'lie',\n",
       "  'nation',\n",
       "  'panem',\n",
       "  'shine',\n",
       "  'capitol',\n",
       "  'surround',\n",
       "  'twelv',\n",
       "  'outli',\n",
       "  'district',\n",
       "  'the',\n",
       "  'capitol',\n",
       "  'harsh',\n",
       "  'cruel',\n",
       "  'keep',\n",
       "  'district',\n",
       "  'line',\n",
       "  'forc',\n",
       "  'send',\n",
       "  'one',\n",
       "  'boy',\n",
       "  'one',\n",
       "  'girl',\n",
       "  'age',\n",
       "  'could',\n",
       "  'surviv',\n",
       "  'wild',\n",
       "  'everi',\n",
       "  'one',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'live',\n",
       "  'see',\n",
       "  'morn',\n",
       "  'In',\n",
       "  'ruin',\n",
       "  'place',\n",
       "  'known',\n",
       "  'north',\n",
       "  'america',\n",
       "  'lie',\n",
       "  'nation',\n",
       "  'panem',\n",
       "  'shine',\n",
       "  'capitol',\n",
       "  'surround',\n",
       "  'twelv',\n",
       "  'outli',\n",
       "  'district',\n",
       "  'the',\n",
       "  'capitol',\n",
       "  'harsh',\n",
       "  'cruel',\n",
       "  'keep',\n",
       "  'district',\n",
       "  'line',\n",
       "  'forc',\n",
       "  'send',\n",
       "  'one',\n",
       "  'boy',\n",
       "  'one',\n",
       "  'girl',\n",
       "  'age',\n",
       "  'twelv',\n",
       "  'eighteen',\n",
       "  'particip',\n",
       "  'annual',\n",
       "  'hunger',\n",
       "  'game',\n",
       "  'fight',\n",
       "  'death',\n",
       "  'live',\n",
       "  'katniss',\n",
       "  'everdeen',\n",
       "  'live',\n",
       "  'alon',\n",
       "  'mother',\n",
       "  'younger',\n",
       "  'sister',\n",
       "  'regard',\n",
       "  'death',\n",
       "  'sentenc',\n",
       "  'step',\n",
       "  'forward',\n",
       "  'take',\n",
       "  'sister',\n",
       "  'place',\n",
       "  'game',\n",
       "  'but',\n",
       "  'katniss',\n",
       "  'close',\n",
       "  'dead',\n",
       "  'surviv',\n",
       "  'second',\n",
       "  'natur',\n",
       "  'without',\n",
       "  'realli',\n",
       "  'mean',\n",
       "  'becom',\n",
       "  'contend',\n",
       "  'but',\n",
       "  'win',\n",
       "  'start',\n",
       "  'make',\n",
       "  'choic',\n",
       "  'weight',\n",
       "  'surviv',\n",
       "  'human',\n",
       "  'life',\n",
       "  'love'],\n",
       " ['harri',\n",
       "  'potter',\n",
       "  'midway',\n",
       "  'train',\n",
       "  'wizard',\n",
       "  'come',\n",
       "  'age',\n",
       "  'harri',\n",
       "  'want',\n",
       "  'get',\n",
       "  'away',\n",
       "  'pernici',\n",
       "  'dursley',\n",
       "  'go',\n",
       "  'intern',\n",
       "  'quidditch',\n",
       "  'cup',\n",
       "  'hermion',\n",
       "  'ron',\n",
       "  'weasley',\n",
       "  'He',\n",
       "  'want',\n",
       "  'dream',\n",
       "  'cho',\n",
       "  'chang',\n",
       "  'crush',\n",
       "  'mayb',\n",
       "  'dream',\n",
       "  'He',\n",
       "  'want',\n",
       "  'find',\n",
       "  'mysteri',\n",
       "  'event',\n",
       "  'suppos',\n",
       "  'take',\n",
       "  'place',\n",
       "  'hogwa',\n",
       "  'harri',\n",
       "  'potter',\n",
       "  'midway',\n",
       "  'train',\n",
       "  'wizard',\n",
       "  'come',\n",
       "  'age',\n",
       "  'harri',\n",
       "  'want',\n",
       "  'get',\n",
       "  'away',\n",
       "  'pernici',\n",
       "  'dursley',\n",
       "  'go',\n",
       "  'intern',\n",
       "  'quidditch',\n",
       "  'cup',\n",
       "  'hermion',\n",
       "  'ron',\n",
       "  'weasley',\n",
       "  'He',\n",
       "  'want',\n",
       "  'dream',\n",
       "  'cho',\n",
       "  'chang',\n",
       "  'crush',\n",
       "  'mayb',\n",
       "  'dream',\n",
       "  'He',\n",
       "  'want',\n",
       "  'find',\n",
       "  'mysteri',\n",
       "  'event',\n",
       "  'suppos',\n",
       "  'take',\n",
       "  'place',\n",
       "  'hogwart',\n",
       "  'year',\n",
       "  'event',\n",
       "  'involv',\n",
       "  'two',\n",
       "  'rival',\n",
       "  'school',\n",
       "  'magic',\n",
       "  'competit',\n",
       "  'happen',\n",
       "  'hundr',\n",
       "  'year',\n",
       "  'He',\n",
       "  'want',\n",
       "  'normal',\n",
       "  'wizard',\n",
       "  'but',\n",
       "  'unfortun',\n",
       "  'harri',\n",
       "  'potter',\n",
       "  'normal',\n",
       "  'even',\n",
       "  'wizard',\n",
       "  'case',\n",
       "  'differ',\n",
       "  'deadli']]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We do the preprocessing and the creation of the vocabulary in one go\n",
    "vocabulary = []\n",
    "\n",
    "# start looping over the tsv files\n",
    "for x in range(1,5):\n",
    "\n",
    "    data = pd.read_csv(\"html_tsv_test_\"+str(x)+\".tsv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "    plot = data['Plot'].item()\n",
    "    \n",
    "    #tokenisation\n",
    "    word_tokens = word_tokenize(plot) \n",
    "    # stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filter_stopsword = [w for w in word_tokens if not w in stop_words]\n",
    "    # punctuations\n",
    "    filter_punc = [w for w in filter_stopsword if w.isalnum()]\n",
    "    # stemming\n",
    "    ps = PorterStemmer() \n",
    "    filter_stem = [ps.stem(w) for w in filter_punc]\n",
    "    # single string\n",
    "    clean_plot = \" \".join(filter_stem)\n",
    "    \n",
    "    # THIS PART TO REPLACE THE OLD PLOT\n",
    "    # replace the old plot with the new one\n",
    "    data[\"Plot\"] = data[\"Plot\"].replace(data['Plot'].item(),clean_plot)\n",
    "    # ideally we will save over the old file\n",
    "    data.to_csv(\"article_\"+str(x)+\".tsv\", sep = '\\t')\n",
    "    \n",
    "    # THIS PART TO CREATE THE VOCABULARY\n",
    "    vocabulary.append(filter_stem)\n",
    "    \n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 1, 'power': 2, 'novel': 3, 'spiritu': 4, 'bond': 5, 'man': 6, 'anim': 7, 'there': 8, 'door': 9, 'end': 10, 'silent': 11, 'corridor': 12, 'and': 13, 'haunt': 14, 'harri': 15, 'pottter': 16, 'dream': 17, 'whi': 18, 'els': 19, 'would': 20, 'wake': 21, 'middl': 22, 'night': 23, 'scream': 24, 'terror': 25, 'lot': 26, 'mind': 27, 'fifth': 28, 'year': 29, 'hogwart': 30, 'defens': 31, 'against': 32, 'dark': 33, 'art': 34, 'teacher': 35, 'person': 36, 'like': 37, 'poison': 38, 'honey': 39, 'big': 40, 'surpris': 41, 'gryffindor': 42, 'quidditch': 43, 'team': 44, 'loomi': 45, 'loom': 46, 'ordinari': 47, 'wizard': 48, 'level': 49, 'exam': 50, 'but': 51, 'thing': 52, 'pale': 53, 'next': 54, 'grow': 55, 'threat': 56, 'neither': 57, 'magic': 58, 'govern': 59, 'author': 60, 'grasp': 61, 'tighten': 62, 'must': 63, 'discov': 64, 'true': 65, 'depth': 66, 'strength': 67, 'friend': 68, 'import': 69, 'boundless': 70, 'loyalti': 71, 'shock': 72, 'price': 73, 'unbear': 74, 'fate': 75, 'depend': 76, 'could': 77, 'surviv': 78, 'wild': 79, 'everi': 80, 'one': 81, 'make': 82, 'sure': 83, 'live': 84, 'see': 85, 'morn': 86, 'In': 87, 'ruin': 88, 'place': 89, 'known': 90, 'north': 91, 'america': 92, 'lie': 93, 'nation': 94, 'panem': 95, 'shine': 96, 'capitol': 97, 'surround': 98, 'twelv': 99, 'outli': 100, 'district': 101, 'the': 102, 'harsh': 103, 'cruel': 104, 'keep': 105, 'line': 106, 'forc': 107, 'send': 108, 'boy': 109, 'girl': 110, 'age': 111, 'eighteen': 112, 'particip': 113, 'annual': 114, 'hunger': 115, 'game': 116, 'fight': 117, 'death': 118, 'katniss': 119, 'everdeen': 120, 'alon': 121, 'mother': 122, 'younger': 123, 'sister': 124, 'regard': 125, 'sentenc': 126, 'step': 127, 'forward': 128, 'take': 129, 'close': 130, 'dead': 131, 'second': 132, 'natur': 133, 'without': 134, 'realli': 135, 'mean': 136, 'becom': 137, 'contend': 138, 'win': 139, 'start': 140, 'choic': 141, 'weight': 142, 'human': 143, 'life': 144, 'love': 145, 'potter': 146, 'midway': 147, 'train': 148, 'come': 149, 'want': 150, 'get': 151, 'away': 152, 'pernici': 153, 'dursley': 154, 'go': 155, 'intern': 156, 'cup': 157, 'hermion': 158, 'ron': 159, 'weasley': 160, 'He': 161, 'cho': 162, 'chang': 163, 'crush': 164, 'mayb': 165, 'find': 166, 'mysteri': 167, 'event': 168, 'suppos': 169, 'hogwa': 170, 'involv': 171, 'two': 172, 'rival': 173, 'school': 174, 'competit': 175, 'happen': 176, 'hundr': 177, 'normal': 178, 'unfortun': 179, 'even': 180, 'case': 181, 'differ': 182, 'deadli': 183}\n"
     ]
    }
   ],
   "source": [
    "# the Vocabulary is a list of list, we flatten that shit\n",
    "flat_vocabulary = [item for sublist in vocabulary for item in sublist]\n",
    "# putting the elements in a dico will eliminate the duplicates\n",
    "dico_voc = dict.fromkeys(flat_vocabulary)\n",
    "\n",
    "# We get the \"integers\" are asked to do\n",
    "i = 1\n",
    "for k,v in dico_voc.items():\n",
    "    dico_voc[k] = i\n",
    "    i = i +1\n",
    "\n",
    "print(dico_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Create your index\n",
    "\n",
    "In order to create this index we must loop for EACH word in ALLLLLLLL the tsv files we have. This will take time and undoubtedly crash a few times. \n",
    "\n",
    "We start this horrible looping.\n",
    "For each word in the vocabulary -> we loop through each plot -> if the word is in that plot we retrieve we get the \"document_id\"\n",
    "\"where document_i is the id of a document that contains the word.\"\n",
    "\n",
    "now...do documents have ids? -> We MUST align the article_id.tsv with the document_id in this dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ['article_1'], 'power': ['article_1'], 'novel': ['article_1'], 'spiritu': ['article_1'], 'bond': ['article_1'], 'man': ['article_1', 'article_3'], 'anim': ['article_1'], 'there': ['article_2'], 'door': ['article_2'], 'end': ['article_2', 'article_3'], 'silent': ['article_2'], 'corridor': ['article_2'], 'and': ['article_2'], 'haunt': ['article_2'], 'harri': ['article_2', 'article_4'], 'pottter': ['article_2'], 'dream': ['article_2', 'article_4'], 'whi': ['article_2'], 'els': ['article_2'], 'would': ['article_2'], 'wake': ['article_2'], 'middl': ['article_2'], 'night': ['article_2'], 'scream': ['article_2'], 'terror': ['article_2'], 'lot': ['article_2'], 'mind': ['article_2'], 'fifth': ['article_2'], 'year': ['article_2', 'article_4'], 'hogwart': ['article_2', 'article_4'], 'defens': ['article_2'], 'against': ['article_2'], 'dark': ['article_2'], 'art': ['article_2', 'article_3', 'article_4'], 'teacher': ['article_2'], 'person': ['article_2'], 'like': ['article_2'], 'poison': ['article_2'], 'honey': ['article_2'], 'big': ['article_2'], 'surpris': ['article_2'], 'gryffindor': ['article_2'], 'quidditch': ['article_2', 'article_4'], 'team': ['article_2'], 'loomi': ['article_2'], 'loom': ['article_2'], 'ordinari': ['article_2'], 'wizard': ['article_2', 'article_4'], 'level': ['article_2'], 'exam': ['article_2'], 'but': ['article_2', 'article_3', 'article_4'], 'thing': ['article_2'], 'pale': ['article_2'], 'next': ['article_2'], 'grow': ['article_2'], 'threat': ['article_2'], 'neither': ['article_2'], 'magic': ['article_2', 'article_4'], 'govern': ['article_2'], 'author': ['article_2'], 'grasp': ['article_2'], 'tighten': ['article_2'], 'must': ['article_2'], 'discov': ['article_2'], 'true': ['article_2'], 'depth': ['article_2'], 'strength': ['article_2'], 'friend': ['article_2'], 'import': ['article_2'], 'boundless': ['article_2'], 'loyalti': ['article_2'], 'shock': ['article_2'], 'price': ['article_2'], 'unbear': ['article_2'], 'fate': ['article_2'], 'depend': ['article_2'], 'could': ['article_3'], 'surviv': ['article_3'], 'wild': ['article_3'], 'everi': ['article_3'], 'one': ['article_2', 'article_3'], 'make': ['article_3'], 'sure': ['article_3'], 'live': ['article_3'], 'see': ['article_3'], 'morn': ['article_3'], 'In': ['article_3'], 'ruin': ['article_3'], 'place': ['article_3', 'article_4'], 'known': ['article_3'], 'north': ['article_3'], 'america': ['article_3'], 'lie': ['article_3'], 'nation': ['article_3'], 'panem': ['article_3'], 'shine': ['article_3'], 'capitol': ['article_3'], 'surround': ['article_3'], 'twelv': ['article_3'], 'outli': ['article_3'], 'district': ['article_3'], 'the': ['article_2', 'article_3'], 'harsh': ['article_3'], 'cruel': ['article_3'], 'keep': ['article_3'], 'line': ['article_3'], 'forc': ['article_3'], 'send': ['article_3'], 'boy': ['article_3'], 'girl': ['article_3'], 'age': ['article_3', 'article_4'], 'eighteen': ['article_3'], 'particip': ['article_3'], 'annual': ['article_3'], 'hunger': ['article_3'], 'game': ['article_3'], 'fight': ['article_3'], 'death': ['article_3'], 'katniss': ['article_3'], 'everdeen': ['article_3'], 'alon': ['article_3'], 'mother': ['article_3'], 'younger': ['article_3'], 'sister': ['article_3'], 'regard': ['article_3'], 'sentenc': ['article_3'], 'step': ['article_3'], 'forward': ['article_3'], 'take': ['article_3', 'article_4'], 'close': ['article_3'], 'dead': ['article_3', 'article_4'], 'second': ['article_3'], 'natur': ['article_3'], 'without': ['article_3'], 'realli': ['article_3'], 'mean': ['article_3'], 'becom': ['article_3'], 'contend': ['article_3'], 'win': ['article_3'], 'start': ['article_3'], 'choic': ['article_3'], 'weight': ['article_3'], 'human': ['article_3'], 'life': ['article_3'], 'love': ['article_3'], 'potter': ['article_4'], 'midway': ['article_4'], 'train': ['article_4'], 'come': ['article_4'], 'want': ['article_4'], 'get': ['article_4'], 'away': ['article_4'], 'pernici': ['article_4'], 'dursley': ['article_4'], 'go': ['article_2', 'article_4'], 'intern': ['article_4'], 'cup': ['article_4'], 'hermion': ['article_4'], 'ron': ['article_4'], 'weasley': ['article_4'], 'He': ['article_4'], 'cho': ['article_3', 'article_4'], 'chang': ['article_4'], 'crush': ['article_4'], 'mayb': ['article_4'], 'find': ['article_2', 'article_4'], 'mysteri': ['article_4'], 'event': ['article_4'], 'suppos': ['article_4'], 'hogwa': ['article_2', 'article_4'], 'involv': ['article_4'], 'two': ['article_4'], 'rival': ['article_4'], 'school': ['article_4'], 'competit': ['article_4'], 'happen': ['article_4'], 'hundr': ['article_4'], 'normal': ['article_4'], 'unfortun': ['article_4'], 'even': ['article_4'], 'case': ['article_4'], 'differ': ['article_4'], 'deadli': ['article_4']}\n"
     ]
    }
   ],
   "source": [
    "index = {}\n",
    "\n",
    "for k,v in dico_voc.items():\n",
    "    for x in range(1,5):\n",
    "        with open(\"article_\"+str(x)+\".tsv\", \"r\") as article:\n",
    "            data = pd.read_csv(article, sep = \"\\t\", encoding = \"utf-8\")\n",
    "            if k in data[\"Plot\"].item():\n",
    "                if k not in index:\n",
    "                    # this would require to be adjusted if the id is just an integer\n",
    "                    index[k] = [\"article_\"+str(x)]\n",
    "                elif k in index:\n",
    "                    index[k].append(\"article_\"+str(x))\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [11, 1], 'b': 2, 'd': 3}\n"
     ]
    }
   ],
   "source": [
    "# Easier example for the one above. in case the logic needs to be re-examined\n",
    "index_test = {\"a\":[11]}\n",
    "\n",
    "voc = {\"a\":1, \"b\":2, \"d\":3}\n",
    "\n",
    "for k,v in voc.items():\n",
    "    if k not in index_test:\n",
    "        index_test[k] = v\n",
    "    else:\n",
    "        index_test[k].append(v)\n",
    "\n",
    "print(index_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power\n"
     ]
    }
   ],
   "source": [
    "x = input()\n",
    "split_input = x.split()\n",
    "\n",
    "# my guess is that we should also do the pre-processing here. otherwise we if we ask for \"harry\" we get no response\n",
    "# that is because \"harry\" transformed into \"harri\" following the pre-processing. \n",
    "\n",
    "sets = []\n",
    "for y in split_input:\n",
    "    if y in index:\n",
    "        sets.append(index[y])\n",
    "\n",
    "\n",
    "# What we want is the intersection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['article_1']]\n"
     ]
    }
   ],
   "source": [
    "print(sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
