{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from lxml import html\n",
    "import requests\n",
    "from langdetect import detect\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.1 -> Getting all the URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be changed to 201 to 301\n",
    "outfile = open(\"test.txt\", \"w\")\n",
    "for i in range(201,301):\n",
    "    \n",
    "    base_url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(i)\n",
    "\n",
    "    html = urlopen(base_url)\n",
    "    soup = BeautifulSoup(html.read(), features=\"lxml\")\n",
    "\n",
    "    links = []\n",
    "    regex = \"/book/show/.\"\n",
    "\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        if re.match(regex, str(link.get(\"href\"))):\n",
    "            links.append(\"https://www.goodreads.com\"+link.get(\"href\"))\n",
    "    \n",
    "    #issues of duplicates        \n",
    "    links = list(dict.fromkeys(links))\n",
    "    \n",
    "    for link in links:\n",
    "        outfile.write(link+\"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.2 -> getting the HTML from the URLs, getting these HTMLs in different files\n",
    "\n",
    "do not run the code below, this is just to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "# First, I make sure i am in the directory that has the text file with the URLS\n",
    "path = \"C:/Users/Guillaume/Desktop/ADM_HM3\"\n",
    "os.chdir(path)\n",
    "\n",
    "i = 1\n",
    "page = 1\n",
    "\n",
    "with open(\"test_urls.txt\", \"r\") as urls:\n",
    "    # I previously created the 100 directories, from page 201 to page 300\n",
    "    # I change the working directory for which i want the HTMLs \n",
    "    path = \"C:/Users/Guillaume/Desktop/ADM_HM3/page_201\"\n",
    "    os.chdir(path)\n",
    "\n",
    "    for url in urls.readlines()[0:]:\n",
    "\n",
    "        html = urlopen(url)\n",
    "\n",
    "        soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "        \n",
    "        # The logig here is not the best but it works- \n",
    "        # Basically, once every 100 urls treated I chance the directory\n",
    "        \n",
    "        if (i%101) != 0:\n",
    "                # We stay in the current directory\n",
    "            with open(\"article_\"+str(i)+\".html\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "                outfile.write(str(soup))\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        else :\n",
    "\n",
    "            page = page + 1\n",
    "            path = \"C:/Users/Guillaume/Desktop/ADM_HM3/page_20\"+str(page)\n",
    "            os.chdir(path)\n",
    "\n",
    "            with open(\"article_\"+str(i)+\".html\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "                outfile.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has been done locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1.3 -> Parsing the HTMLs files to retrieve all the required info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All functions to retrieve the informations\n",
    "\n",
    "def get_title(soup):\n",
    "    title = soup.find(id=\"bookTitle\").contents[0].strip()\n",
    "    return title\n",
    "\n",
    "def get_series(soup):\n",
    "    serie = soup.find(id=\"bookSeries\").get_text().strip().replace(\"(\",\"\").replace(\")\",\"\")\n",
    "    regex = \"#.*\"\n",
    "    serie = re.sub(regex, \"\",serie)\n",
    "    return serie\n",
    "\n",
    "def get_author(soup):\n",
    "    author = soup.find_all(class_=\"authorName__container\")\n",
    "    author_list = \" \".join([x.get_text().strip() for x in author])\n",
    "    return author_list\n",
    "\n",
    "def get_pages(soup):\n",
    "    pages = soup.find(itemprop =\"numberOfPages\").get_text().split()[0]\n",
    "    return int(pages)\n",
    "\n",
    "def get_ratingValue(soup):\n",
    "    ratingValue = soup.find(itemprop =\"ratingValue\").get_text().split()[0]\n",
    "    return float(ratingValue)\n",
    "\n",
    "def get_ratingCount(soup):\n",
    "    test = []\n",
    "    ratingCount = soup.find_all(class_ =\"gr-hyperlink\", href=\"#other_reviews\")\n",
    "    for x in ratingCount:\n",
    "        test.append(x.get_text().split())\n",
    "    return int(test[0][0].replace(\",\",\"\"))\n",
    "    \n",
    "def get_reviewCount(soup):\n",
    "    test = []\n",
    "    ratingCount = soup.find_all(class_ =\"gr-hyperlink\", href=\"#other_reviews\")\n",
    "    for x in ratingCount:\n",
    "        test.append(x.get_text().split())\n",
    "    return int(test[1][0].replace(\",\",\"\"))\n",
    "\n",
    "def get_date(soup):\n",
    "    regex = \"by.*\"\n",
    "    date = soup.find_all(class_ = \"row\")\n",
    "    date = re.sub(regex,\"\" ,str(date[1].contents[0]).replace(\"Published\",\"\")).strip()\n",
    "    return date\n",
    "\n",
    "def get_setting(soup):\n",
    "    string = re.compile(\"/places/.\")\n",
    "    test = soup.find_all(href=string)\n",
    "    \n",
    "    places_list = \" \".join([(str(a.contents[0])) for a in test])\n",
    "\n",
    "    return places_list\n",
    "\n",
    "def get_plot(soup):\n",
    "    \n",
    "    plot = soup.find(id=\"description\")\n",
    "    plot = plot.find_all(id= re.compile(\"freeText\\.*\"))\n",
    "\n",
    "    final_plot = [] # It seems i need a list because joining on an empty string gives me a !\"Â£$%&\"@ blank\n",
    "    \n",
    "    # now there should not be a case where there are 3 span tags ...hopefully\n",
    "    if len(plot) > 1:\n",
    "        final_plot.append(plot[1].get_text())\n",
    "    else:\n",
    "        final_plot.append(plot[0].get_text())\n",
    "\n",
    "    return final_plot[0]\n",
    "\n",
    "def get_char(soup):\n",
    "    string = re.compile(\"/characters/.\")\n",
    "    test = soup.find_all(href=string)\n",
    "\n",
    "    char_list= \" \".join([(str(a.contents[0])) for a in test])\n",
    "\n",
    "    return char_list\n",
    "\n",
    "def get_url(soup):\n",
    "\n",
    "    regex = \"https://www.goodreads.com/book/show/.\"\n",
    "    ratingCount = soup.find(href =re.compile(regex), rel=\"canonical\")\n",
    "    return ratingCount.get('href')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want 1 TSV file for 1 article\n",
    "# In order to do this -> we first loop over the HTML\n",
    "# We check if the language is indeed english\n",
    "# for each HTML we create a new TSV and input the required DATA\n",
    "\n",
    "i = 1\n",
    "\n",
    "while i <= 4:\n",
    "    html = open(\"article_\"+str(i)+\".html\", \"r\", encoding=\"utf-8\")\n",
    "    soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "        \n",
    "    with open(\"html_tsv_test_\"+str(i)+\".tsv\", 'w', encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        # We check the language of the splot\n",
    "        if detect(get_plot(soup)) == \"en\":\n",
    "            \n",
    "            test_list_tsv = [get_title(soup), \n",
    "                             get_series(soup), \n",
    "                             get_author(soup),\n",
    "                             get_ratingValue(soup),\n",
    "                             get_ratingCount(soup), \n",
    "                             get_reviewCount(soup),\n",
    "                             get_plot(soup),\n",
    "                             get_pages(soup),\n",
    "                             get_date(soup),\n",
    "                             get_setting(soup),\n",
    "                             get_char(soup),\n",
    "                             get_url(soup)]\n",
    "            \n",
    "            header_list = [\"bookTitle\",\"bookSeries\",\"bookAuthors\",\"ratingValue\",\"ratingCount\",\"reviewCount\",\"Plot\",\"NumberofPages\",\"Date\",\"Characters\",\"Setting\",\"URL\"]\n",
    "\n",
    "            tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "            tsv_output.writerow(header_list)\n",
    "            tsv_output.writerow(test_list_tsv)\n",
    "            \n",
    "            i = i + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "- Removing stopwords\n",
    "- Removing punctuation\n",
    "- Stemming\n",
    "- Anything else you think it's needed ?\n",
    "\n",
    "What is \"anything else\"?\n",
    " - lower, upper case ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the preprocessing and the creation of the vocabulary in one go\n",
    "word_list = []\n",
    "\n",
    "# start looping over the tsv files\n",
    "for x in range(1,5):\n",
    "\n",
    "    data = pd.read_csv(\"html_tsv_test_\"+str(x)+\".tsv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "    plot = data['Plot'].item()\n",
    "    \n",
    "    #tokenisation\n",
    "    word_tokens = word_tokenize(plot) \n",
    "    # stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filter_stopsword = [w for w in word_tokens if not w in stop_words]\n",
    "    # punctuations\n",
    "    filter_punc = [w for w in filter_stopsword if w.isalnum()]\n",
    "    # stemming\n",
    "    ps = PorterStemmer() \n",
    "    filter_stem = [ps.stem(w) for w in filter_punc]\n",
    "    # single string\n",
    "    clean_plot = \" \".join(filter_stem)\n",
    "    \n",
    "    # THIS PART TO REPLACE THE OLD PLOT\n",
    "    # replace the old plot with the new one\n",
    "    data[\"Plot\"] = data[\"Plot\"].replace(data['Plot'].item(),clean_plot)\n",
    "    # ideally we will save over the old file\n",
    "    data.to_csv(\"article_\"+str(x)+\".tsv\", sep = '\\t')\n",
    "    \n",
    "    # THIS PART TO CREATE THE VOCABULARY\n",
    "    word_list.append(filter_stem)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'power': 1,\n",
       " 'novel': 2,\n",
       " 'spiritu': 3,\n",
       " 'bond': 4,\n",
       " 'man': 5,\n",
       " 'anim': 6,\n",
       " 'there': 7,\n",
       " 'door': 8,\n",
       " 'end': 9,\n",
       " 'silent': 10,\n",
       " 'corridor': 11,\n",
       " 'and': 12,\n",
       " 'haunt': 13,\n",
       " 'harri': 14,\n",
       " 'pottter': 15,\n",
       " 'dream': 16,\n",
       " 'whi': 17,\n",
       " 'els': 18,\n",
       " 'would': 19,\n",
       " 'wake': 20,\n",
       " 'middl': 21,\n",
       " 'night': 22,\n",
       " 'scream': 23,\n",
       " 'terror': 24,\n",
       " 'lot': 25,\n",
       " 'mind': 26,\n",
       " 'fifth': 27,\n",
       " 'year': 28,\n",
       " 'hogwart': 29,\n",
       " 'defens': 30,\n",
       " 'against': 31,\n",
       " 'dark': 32,\n",
       " 'art': 33,\n",
       " 'teacher': 34,\n",
       " 'person': 35,\n",
       " 'like': 36,\n",
       " 'poison': 37,\n",
       " 'honey': 38,\n",
       " 'big': 39,\n",
       " 'surpris': 40,\n",
       " 'gryffindor': 41,\n",
       " 'quidditch': 42,\n",
       " 'team': 43,\n",
       " 'loom': 44,\n",
       " 'ordinari': 45,\n",
       " 'wizard': 46,\n",
       " 'level': 47,\n",
       " 'exam': 48,\n",
       " 'but': 49,\n",
       " 'thing': 50,\n",
       " 'pale': 51,\n",
       " 'next': 52,\n",
       " 'grow': 53,\n",
       " 'threat': 54,\n",
       " 'neither': 55,\n",
       " 'magic': 56,\n",
       " 'govern': 57,\n",
       " 'author': 58,\n",
       " 'grasp': 59,\n",
       " 'tighten': 60,\n",
       " 'must': 61,\n",
       " 'discov': 62,\n",
       " 'true': 63,\n",
       " 'depth': 64,\n",
       " 'strength': 65,\n",
       " 'friend': 66,\n",
       " 'import': 67,\n",
       " 'boundless': 68,\n",
       " 'loyalti': 69,\n",
       " 'shock': 70,\n",
       " 'price': 71,\n",
       " 'unbear': 72,\n",
       " 'fate': 73,\n",
       " 'depend': 74,\n",
       " 'could': 75,\n",
       " 'surviv': 76,\n",
       " 'wild': 77,\n",
       " 'everi': 78,\n",
       " 'one': 79,\n",
       " 'make': 80,\n",
       " 'sure': 81,\n",
       " 'live': 82,\n",
       " 'see': 83,\n",
       " 'morn': 84,\n",
       " 'In': 85,\n",
       " 'ruin': 86,\n",
       " 'place': 87,\n",
       " 'known': 88,\n",
       " 'north': 89,\n",
       " 'america': 90,\n",
       " 'lie': 91,\n",
       " 'nation': 92,\n",
       " 'panem': 93,\n",
       " 'shine': 94,\n",
       " 'capitol': 95,\n",
       " 'surround': 96,\n",
       " 'twelv': 97,\n",
       " 'outli': 98,\n",
       " 'district': 99,\n",
       " 'the': 100,\n",
       " 'harsh': 101,\n",
       " 'cruel': 102,\n",
       " 'keep': 103,\n",
       " 'line': 104,\n",
       " 'forc': 105,\n",
       " 'send': 106,\n",
       " 'boy': 107,\n",
       " 'girl': 108,\n",
       " 'age': 109,\n",
       " 'eighteen': 110,\n",
       " 'particip': 111,\n",
       " 'annual': 112,\n",
       " 'hunger': 113,\n",
       " 'game': 114,\n",
       " 'fight': 115,\n",
       " 'death': 116,\n",
       " 'katniss': 117,\n",
       " 'everdeen': 118,\n",
       " 'alon': 119,\n",
       " 'mother': 120,\n",
       " 'younger': 121,\n",
       " 'sister': 122,\n",
       " 'regard': 123,\n",
       " 'sentenc': 124,\n",
       " 'step': 125,\n",
       " 'forward': 126,\n",
       " 'take': 127,\n",
       " 'close': 128,\n",
       " 'dead': 129,\n",
       " 'second': 130,\n",
       " 'natur': 131,\n",
       " 'without': 132,\n",
       " 'realli': 133,\n",
       " 'mean': 134,\n",
       " 'becom': 135,\n",
       " 'contend': 136,\n",
       " 'win': 137,\n",
       " 'start': 138,\n",
       " 'choic': 139,\n",
       " 'weight': 140,\n",
       " 'human': 141,\n",
       " 'life': 142,\n",
       " 'love': 143,\n",
       " 'potter': 144,\n",
       " 'midway': 145,\n",
       " 'train': 146,\n",
       " 'come': 147,\n",
       " 'want': 148,\n",
       " 'get': 149,\n",
       " 'away': 150,\n",
       " 'pernici': 151,\n",
       " 'dursley': 152,\n",
       " 'go': 153,\n",
       " 'intern': 154,\n",
       " 'cup': 155,\n",
       " 'hermion': 156,\n",
       " 'ron': 157,\n",
       " 'weasley': 158,\n",
       " 'He': 159,\n",
       " 'cho': 160,\n",
       " 'chang': 161,\n",
       " 'crush': 162,\n",
       " 'mayb': 163,\n",
       " 'find': 164,\n",
       " 'mysteri': 165,\n",
       " 'event': 166,\n",
       " 'suppos': 167,\n",
       " 'involv': 168,\n",
       " 'two': 169,\n",
       " 'rival': 170,\n",
       " 'school': 171,\n",
       " 'competit': 172,\n",
       " 'happen': 173,\n",
       " 'hundr': 174,\n",
       " 'normal': 175,\n",
       " 'unfortun': 176,\n",
       " 'even': 177,\n",
       " 'case': 178,\n",
       " 'differ': 179,\n",
       " 'deadli': 180}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the Vocabulary is a list of list, we flatten that shit\n",
    "flat_vocabulary = [item for sublist in word_list for item in sublist]\n",
    "# putting the elements in a dico will eliminate the duplicates\n",
    "vocabulary = dict.fromkeys(flat_vocabulary)\n",
    "\n",
    "# We get the \"integers\" we are asked\n",
    "i = 0\n",
    "for k,v in vocabulary.items():\n",
    "    vocabulary[k] = i\n",
    "    i = i +1\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Create your index\n",
    "\n",
    "In order to create this index we must loop for EACH word in ALLLLLLLL the tsv files we have. This will take time and undoubtedly crash a few times. \n",
    "\n",
    "We start this horrible looping.\n",
    "For each word in the vocabulary -> we loop through each plot -> if the word is in that plot we retrieve we get the \"document_id\"\n",
    "\"where document_i is the id of a document that contains the word.\"\n",
    "\n",
    "now...do documents have ids? -> We MUST align the article_id.tsv with the document_id in this dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['article_1'], 1: ['article_1'], 2: ['article_1'], 3: ['article_1'], 4: ['article_1'], 5: ['article_1', 'article_3'], 6: ['article_1'], 7: ['article_2'], 8: ['article_2'], 9: ['article_2', 'article_3'], 10: ['article_2'], 11: ['article_2'], 12: ['article_2'], 13: ['article_2'], 14: ['article_2', 'article_4'], 15: ['article_2'], 16: ['article_2', 'article_4'], 17: ['article_2'], 18: ['article_2'], 19: ['article_2'], 20: ['article_2'], 21: ['article_2'], 22: ['article_2'], 23: ['article_2'], 24: ['article_2'], 25: ['article_2'], 26: ['article_2'], 27: ['article_2'], 28: ['article_2', 'article_4'], 29: ['article_2', 'article_4'], 30: ['article_2'], 31: ['article_2'], 32: ['article_2'], 33: ['article_2', 'article_3', 'article_4'], 34: ['article_2'], 35: ['article_2'], 36: ['article_2'], 37: ['article_2'], 38: ['article_2'], 39: ['article_2'], 40: ['article_2'], 41: ['article_2'], 42: ['article_2', 'article_4'], 43: ['article_2'], 44: ['article_2'], 45: ['article_2'], 46: ['article_2', 'article_4'], 47: ['article_2'], 48: ['article_2'], 49: ['article_2', 'article_3', 'article_4'], 50: ['article_2'], 51: ['article_2'], 52: ['article_2'], 53: ['article_2'], 54: ['article_2'], 55: ['article_2'], 56: ['article_2', 'article_4'], 57: ['article_2'], 58: ['article_2'], 59: ['article_2'], 60: ['article_2'], 61: ['article_2'], 62: ['article_2'], 63: ['article_2'], 64: ['article_2'], 65: ['article_2'], 66: ['article_2'], 67: ['article_2'], 68: ['article_2'], 69: ['article_2'], 70: ['article_2'], 71: ['article_2'], 72: ['article_2'], 73: ['article_2'], 74: ['article_2'], 75: ['article_3'], 76: ['article_3'], 77: ['article_3'], 78: ['article_3'], 79: ['article_2', 'article_3'], 80: ['article_3'], 81: ['article_3'], 82: ['article_3'], 83: ['article_3'], 84: ['article_3'], 85: ['article_3'], 86: ['article_3'], 87: ['article_3', 'article_4'], 88: ['article_3'], 89: ['article_3'], 90: ['article_3'], 91: ['article_3'], 92: ['article_3'], 93: ['article_3'], 94: ['article_3'], 95: ['article_3'], 96: ['article_3'], 97: ['article_3'], 98: ['article_3'], 99: ['article_3'], 100: ['article_2', 'article_3'], 101: ['article_3'], 102: ['article_3'], 103: ['article_3'], 104: ['article_3'], 105: ['article_3'], 106: ['article_3'], 107: ['article_3'], 108: ['article_3'], 109: ['article_3', 'article_4'], 110: ['article_3'], 111: ['article_3'], 112: ['article_3'], 113: ['article_3'], 114: ['article_3'], 115: ['article_3'], 116: ['article_3'], 117: ['article_3'], 118: ['article_3'], 119: ['article_3'], 120: ['article_3'], 121: ['article_3'], 122: ['article_3'], 123: ['article_3'], 124: ['article_3'], 125: ['article_3'], 126: ['article_3'], 127: ['article_3', 'article_4'], 128: ['article_3'], 129: ['article_3', 'article_4'], 130: ['article_3'], 131: ['article_3'], 132: ['article_3'], 133: ['article_3'], 134: ['article_3'], 135: ['article_3'], 136: ['article_3'], 137: ['article_3'], 138: ['article_3'], 139: ['article_3'], 140: ['article_3'], 141: ['article_3'], 142: ['article_3'], 143: ['article_3'], 144: ['article_4'], 145: ['article_4'], 146: ['article_4'], 147: ['article_4'], 148: ['article_4'], 149: ['article_4'], 150: ['article_4'], 151: ['article_4'], 152: ['article_4'], 153: ['article_2', 'article_4'], 154: ['article_4'], 155: ['article_4'], 156: ['article_4'], 157: ['article_4'], 158: ['article_4'], 159: ['article_4'], 160: ['article_3', 'article_4'], 161: ['article_4'], 162: ['article_4'], 163: ['article_4'], 164: ['article_2', 'article_4'], 165: ['article_4'], 166: ['article_4'], 167: ['article_4'], 168: ['article_4'], 169: ['article_4'], 170: ['article_4'], 171: ['article_4'], 172: ['article_4'], 173: ['article_4'], 174: ['article_4'], 175: ['article_4'], 176: ['article_4'], 177: ['article_4'], 178: ['article_4'], 179: ['article_4'], 180: ['article_4']}\n"
     ]
    }
   ],
   "source": [
    "# We now create the inverted index : the format is as follow\n",
    "#{ term_id_1:[document_1, document_2, document_4],\n",
    "# term_id_2:[document_1, document_3, document_5, document_6], ....}\n",
    "\n",
    "index = {}\n",
    "\n",
    "for k,v in vocabulary.items():\n",
    "    # the 5 needs to change to the number of articles + 1\n",
    "    for x in range(1,5):\n",
    "        with open(\"article_\"+str(x)+\".tsv\", \"r\") as article:\n",
    "            data = pd.read_csv(article, sep = \"\\t\", encoding = \"utf-8\")\n",
    "            # this if statement needs to be changed to something like in 2.2\n",
    "            if k in data[\"Plot\"].item():\n",
    "                if v not in index:\n",
    "                    # Now i dont fucking get why we need these fucking integers when words are perfectly fine\n",
    "                    index[v] = [\"article_\"+str(x)]\n",
    "                elif v in index:\n",
    "                    index[v].append(\"article_\"+str(x))\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harri year\n",
      "[14, 28]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['article_2', 'article_4']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This needs to be changed to a \"search engine\" system. \n",
    "\n",
    "# We ask the user for input\n",
    "query = input()\n",
    "# we split each word of the query\n",
    "query_words = query.split()\n",
    "\n",
    "# We map the word of the query to their integers.\n",
    "word_to_int = []\n",
    "for word in query_words:\n",
    "    # will have to check if that \"in\" works as intended\n",
    "    if word in vocabulary:\n",
    "        word_to_int.append(vocabulary[word])\n",
    "\n",
    "# We ONLY want the articles for which ALL query words are present in the plot\n",
    "\n",
    "sets = []\n",
    "for y in word_to_int:\n",
    "    # Does 12 in 120 return true?\n",
    "    if y in index:\n",
    "        sets.append(set(index[y]))\n",
    "\n",
    "# We look for the intersection of articles\n",
    "sets = [*set.intersection(*sets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfi is the count of documents where the term i is present\n",
    "# We initiate a new dico, keeping the word and starting the count at 0\n",
    "dfi_dico = {}\n",
    "\n",
    "for k,v in vocabulary.items():\n",
    "    dfi_dico[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logic is as followed. For each word in the dictionary, open all the documents, look if it appears at least once.\n",
    "# don't exactly remember why the dico \"fuckthis\" is needed. \n",
    "fuckthis= {}\n",
    "\n",
    "for k, v in dfi_dico.items():\n",
    "    dfi = 0\n",
    "    # This needs to be replaced to the nb of articles + 1\n",
    "    for x in range(1,5):\n",
    "        with open(\"article_\"+str(x)+\".tsv\", \"r\") as article:\n",
    "            data = pd.read_csv(article, sep = \"\\t\", encoding = \"utf-8\")\n",
    "            \n",
    "        \n",
    "            if (data[\"Plot\"].item().split().count(k)) > 0:\n",
    "                dfi += 1\n",
    "    fuckthis[k] = dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1.3862943611198906),\n",
       "  (1, 1.3862943611198906),\n",
       "  (2, 1.3862943611198906),\n",
       "  (3, 1.3862943611198906),\n",
       "  (4, 1.3862943611198906),\n",
       "  (5, 1.3862943611198906),\n",
       "  (6, 1.3862943611198906)],\n",
       " [(7, 1.3862943611198906),\n",
       "  (8, 1.3862943611198906),\n",
       "  (9, 0.46209812037329684),\n",
       "  (10, 1.3862943611198906),\n",
       "  (11, 1.3862943611198906),\n",
       "  (12, 1.3862943611198906),\n",
       "  (13, 1.3862943611198906),\n",
       "  (14, 0.23104906018664842),\n",
       "  (15, 1.3862943611198906),\n",
       "  (16, 0.6931471805599453),\n",
       "  (17, 1.3862943611198906),\n",
       "  (18, 1.3862943611198906),\n",
       "  (19, 1.3862943611198906),\n",
       "  (20, 1.3862943611198906),\n",
       "  (21, 1.3862943611198906),\n",
       "  (22, 1.3862943611198906),\n",
       "  (23, 1.3862943611198906),\n",
       "  (24, 0.6931471805599453),\n",
       "  (14, 0.23104906018664842),\n",
       "  (25, 1.3862943611198906),\n",
       "  (26, 1.3862943611198906),\n",
       "  (27, 1.3862943611198906),\n",
       "  (28, 0.6931471805599453),\n",
       "  (29, 0.34657359027997264),\n",
       "  (30, 1.3862943611198906),\n",
       "  (31, 1.3862943611198906),\n",
       "  (32, 0.6931471805599453),\n",
       "  (33, 0.46209812037329684),\n",
       "  (34, 1.3862943611198906),\n",
       "  (35, 1.3862943611198906),\n",
       "  (36, 1.3862943611198906),\n",
       "  (37, 1.3862943611198906),\n",
       "  (38, 1.3862943611198906),\n",
       "  (39, 1.3862943611198906),\n",
       "  (40, 1.3862943611198906),\n",
       "  (41, 1.3862943611198906),\n",
       "  (42, 0.6931471805599453),\n",
       "  (43, 1.3862943611198906),\n",
       "  (44, 1.3862943611198906),\n",
       "  (24, 0.6931471805599453),\n",
       "  (45, 1.3862943611198906),\n",
       "  (46, 0.6931471805599453),\n",
       "  (47, 1.3862943611198906),\n",
       "  (48, 1.3862943611198906),\n",
       "  (49, 0.28768207245178085),\n",
       "  (50, 1.3862943611198906),\n",
       "  (51, 1.3862943611198906),\n",
       "  (52, 1.3862943611198906),\n",
       "  (53, 1.3862943611198906),\n",
       "  (54, 0.6931471805599453),\n",
       "  (54, 0.6931471805599453),\n",
       "  (55, 1.3862943611198906),\n",
       "  (56, 0.6931471805599453),\n",
       "  (57, 1.3862943611198906),\n",
       "  (58, 1.3862943611198906),\n",
       "  (29, 0.34657359027997264),\n",
       "  (59, 1.3862943611198906),\n",
       "  (32, 0.6931471805599453),\n",
       "  (60, 1.3862943611198906),\n",
       "  (14, 0.23104906018664842),\n",
       "  (61, 1.3862943611198906),\n",
       "  (62, 1.3862943611198906),\n",
       "  (63, 1.3862943611198906),\n",
       "  (64, 1.3862943611198906),\n",
       "  (65, 1.3862943611198906),\n",
       "  (66, 1.3862943611198906),\n",
       "  (67, 1.3862943611198906),\n",
       "  (68, 1.3862943611198906),\n",
       "  (69, 1.3862943611198906),\n",
       "  (70, 1.3862943611198906),\n",
       "  (71, 1.3862943611198906),\n",
       "  (72, 1.3862943611198906),\n",
       "  (73, 1.3862943611198906),\n",
       "  (74, 1.3862943611198906)],\n",
       " [(75, 1.3862943611198906),\n",
       "  (76, 0.46209812037329684),\n",
       "  (77, 1.3862943611198906),\n",
       "  (78, 1.3862943611198906),\n",
       "  (79, 0.46209812037329684),\n",
       "  (80, 0.6931471805599453),\n",
       "  (81, 1.3862943611198906),\n",
       "  (82, 0.46209812037329684),\n",
       "  (83, 1.3862943611198906),\n",
       "  (84, 1.3862943611198906),\n",
       "  (85, 1.3862943611198906),\n",
       "  (86, 1.3862943611198906),\n",
       "  (87, 0.34657359027997264),\n",
       "  (88, 1.3862943611198906),\n",
       "  (89, 1.3862943611198906),\n",
       "  (90, 1.3862943611198906),\n",
       "  (91, 1.3862943611198906),\n",
       "  (92, 1.3862943611198906),\n",
       "  (93, 1.3862943611198906),\n",
       "  (94, 1.3862943611198906),\n",
       "  (95, 0.6931471805599453),\n",
       "  (96, 1.3862943611198906),\n",
       "  (97, 0.6931471805599453),\n",
       "  (98, 1.3862943611198906),\n",
       "  (99, 0.6931471805599453),\n",
       "  (100, 0.6931471805599453),\n",
       "  (95, 0.6931471805599453),\n",
       "  (101, 1.3862943611198906),\n",
       "  (102, 1.3862943611198906),\n",
       "  (103, 1.3862943611198906),\n",
       "  (99, 0.6931471805599453),\n",
       "  (104, 1.3862943611198906),\n",
       "  (105, 1.3862943611198906),\n",
       "  (106, 1.3862943611198906),\n",
       "  (79, 0.46209812037329684),\n",
       "  (107, 1.3862943611198906),\n",
       "  (79, 0.46209812037329684),\n",
       "  (108, 1.3862943611198906),\n",
       "  (109, 0.6931471805599453),\n",
       "  (97, 0.6931471805599453),\n",
       "  (110, 1.3862943611198906),\n",
       "  (111, 1.3862943611198906),\n",
       "  (112, 1.3862943611198906),\n",
       "  (113, 1.3862943611198906),\n",
       "  (114, 0.6931471805599453),\n",
       "  (115, 1.3862943611198906),\n",
       "  (116, 0.6931471805599453),\n",
       "  (82, 0.46209812037329684),\n",
       "  (117, 0.6931471805599453),\n",
       "  (118, 1.3862943611198906),\n",
       "  (82, 0.46209812037329684),\n",
       "  (119, 1.3862943611198906),\n",
       "  (120, 1.3862943611198906),\n",
       "  (121, 1.3862943611198906),\n",
       "  (122, 0.6931471805599453),\n",
       "  (123, 1.3862943611198906),\n",
       "  (116, 0.6931471805599453),\n",
       "  (124, 1.3862943611198906),\n",
       "  (125, 1.3862943611198906),\n",
       "  (126, 1.3862943611198906),\n",
       "  (127, 0.6931471805599453),\n",
       "  (122, 0.6931471805599453),\n",
       "  (87, 0.34657359027997264),\n",
       "  (114, 0.6931471805599453),\n",
       "  (49, 0.14384103622589042),\n",
       "  (117, 0.6931471805599453),\n",
       "  (128, 1.3862943611198906),\n",
       "  (129, 1.3862943611198906),\n",
       "  (76, 0.46209812037329684),\n",
       "  (130, 1.3862943611198906),\n",
       "  (131, 1.3862943611198906),\n",
       "  (132, 1.3862943611198906),\n",
       "  (133, 1.3862943611198906),\n",
       "  (134, 1.3862943611198906),\n",
       "  (135, 1.3862943611198906),\n",
       "  (136, 1.3862943611198906),\n",
       "  (49, 0.14384103622589042),\n",
       "  (137, 1.3862943611198906),\n",
       "  (138, 1.3862943611198906),\n",
       "  (80, 0.6931471805599453),\n",
       "  (139, 1.3862943611198906),\n",
       "  (140, 1.3862943611198906),\n",
       "  (76, 0.46209812037329684),\n",
       "  (141, 1.3862943611198906),\n",
       "  (142, 1.3862943611198906),\n",
       "  (143, 1.3862943611198906)],\n",
       " [(14, 0.23104906018664842),\n",
       "  (144, 0.6931471805599453),\n",
       "  (145, 1.3862943611198906),\n",
       "  (146, 1.3862943611198906),\n",
       "  (46, 0.23104906018664842),\n",
       "  (147, 1.3862943611198906),\n",
       "  (109, 0.6931471805599453),\n",
       "  (14, 0.23104906018664842),\n",
       "  (148, 0.34657359027997264),\n",
       "  (149, 1.3862943611198906),\n",
       "  (150, 1.3862943611198906),\n",
       "  (151, 1.3862943611198906),\n",
       "  (152, 1.3862943611198906),\n",
       "  (153, 1.3862943611198906),\n",
       "  (154, 1.3862943611198906),\n",
       "  (42, 0.6931471805599453),\n",
       "  (155, 1.3862943611198906),\n",
       "  (156, 1.3862943611198906),\n",
       "  (157, 1.3862943611198906),\n",
       "  (158, 1.3862943611198906),\n",
       "  (159, 0.46209812037329684),\n",
       "  (148, 0.34657359027997264),\n",
       "  (16, 0.34657359027997264),\n",
       "  (160, 0.6931471805599453),\n",
       "  (161, 1.3862943611198906),\n",
       "  (162, 1.3862943611198906),\n",
       "  (163, 1.3862943611198906),\n",
       "  (16, 0.34657359027997264),\n",
       "  (159, 0.46209812037329684),\n",
       "  (148, 0.34657359027997264),\n",
       "  (164, 1.3862943611198906),\n",
       "  (165, 1.3862943611198906),\n",
       "  (166, 0.6931471805599453),\n",
       "  (167, 1.3862943611198906),\n",
       "  (127, 0.6931471805599453),\n",
       "  (87, 0.6931471805599453),\n",
       "  (29, 0.6931471805599453),\n",
       "  (28, 0.34657359027997264),\n",
       "  (166, 0.6931471805599453),\n",
       "  (168, 1.3862943611198906),\n",
       "  (169, 1.3862943611198906),\n",
       "  (170, 1.3862943611198906),\n",
       "  (171, 1.3862943611198906),\n",
       "  (56, 0.6931471805599453),\n",
       "  (172, 1.3862943611198906),\n",
       "  (173, 1.3862943611198906),\n",
       "  (174, 1.3862943611198906),\n",
       "  (28, 0.34657359027997264),\n",
       "  (159, 0.46209812037329684),\n",
       "  (148, 0.34657359027997264),\n",
       "  (175, 0.6931471805599453),\n",
       "  (46, 0.23104906018664842),\n",
       "  (49, 0.28768207245178085),\n",
       "  (176, 1.3862943611198906),\n",
       "  (14, 0.23104906018664842),\n",
       "  (144, 0.6931471805599453),\n",
       "  (175, 0.6931471805599453),\n",
       "  (177, 0.46209812037329684),\n",
       "  (46, 0.23104906018664842),\n",
       "  (178, 1.3862943611198906),\n",
       "  (179, 1.3862943611198906),\n",
       "  (180, 1.3862943611198906)]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We the reason I want this format -> a list of lists with tuples is because \n",
    "# the code to get the tfidf was created with a library but the it turned out I cannot use this library\n",
    "# so i created a code that would have a similar structure.\n",
    "\n",
    "# finfin because I was tired\n",
    "finfin = []\n",
    "# finfin[0] -> the first article\n",
    "# finfin[0][1] -> the tupple of the word-integer and its tdidf score\n",
    "# finfin[0][1][1] -> the tdidf score for that specific int in that specific article\n",
    "\n",
    "N = 4 # total number of plots/documents\n",
    "# needs to be changed to the nb of articles - 1  or a different loop i dont care\n",
    "for x in range(1,5):\n",
    "    \n",
    "    # tupi for cute tupple. \n",
    "    tupi = []\n",
    "    \n",
    "    with open(\"article_\"+str(x)+\".tsv\", \"r\") as article:\n",
    "        data = pd.read_csv(article, sep = \"\\t\", encoding = \"utf-8\")\n",
    "        \n",
    "        for x in data[\"Plot\"].item().split():\n",
    "            \n",
    "            # frequence of the term i in the document j\n",
    "            tfi = data[\"Plot\"].item().count(x)\n",
    "            \n",
    "            # Weight of Term i in document j - wij\n",
    "            wij = (1/tfi) * math.log(N/fuckthis[x])\n",
    "            \n",
    "            tupi.append((vocabulary[x], wij))\n",
    "    finfin.append((tupi))\n",
    "finfin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logic is the same as before\n",
    "# to get the inverted index, we just check if the word of the dictionary appear in a plot\n",
    "# if it does, we append the article and its tdidf score together.\n",
    "\n",
    "index_2 = {}\n",
    "\n",
    "for k,v in vocabulary.items():\n",
    "    \n",
    "    for x in range(1,5):\n",
    "        \n",
    "        with open(\"article_\"+str(x)+\".tsv\", \"r\") as article:\n",
    "            data = pd.read_csv(article, sep = \"\\t\", encoding = \"utf-8\")\n",
    "            if k in ([x for x in data[\"Plot\"].item().split()]):\n",
    "\n",
    "                if v not in index_2:\n",
    "                    # okej -> it's confusing but the only thing you need to understand is [y[0] for y in finfin[x-1]\n",
    "                    # [y[0] for y in finfin[x-1] -> is just to get the right tupple, remember finfin\n",
    "                    index_2[v] = [(\"article_\"+str(x), finfin[x-1][[y[0] for y in finfin[x-1]].index(v)][1])]\n",
    "                    \n",
    "                elif v in index_2:\n",
    "                    index_2[v].append((\"article_\"+str(x), finfin[x-1][[y[0] for y in finfin[x-1]].index(v)][1]))\n",
    "print(index_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entirely hypothetical\n",
    "Cosine Similary. \n",
    "\n",
    "We asume that A -> the first vector is simply the vector of TFIDF scores we calculed before\n",
    "We assume that B -> the query vector is a vector of the same length of zeros except for the query words\n",
    "\n",
    "A has the preprocessed text : \"epic journey man animal\" --> the TDIDF vector is: \"0.12, 0.56, 0.10, 0.15\"\n",
    "B has the query : \" -   journey -   animal\" --> the TDIDF vector is: \"0   , 0.56, 0   , 0.15\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could use the exact same code as before. this would not change anything.\n",
    "# But i decided to use the inverted index created with the tdidf score\n",
    "\n",
    "\n",
    "x = \"harri year\"\n",
    "split_input = x.split()\n",
    "\n",
    "word_to_int = []\n",
    "for word in split_input:\n",
    "    if word in vocabulary:\n",
    "        word_to_int.append(vocabulary[word])\n",
    "    \n",
    "\n",
    "\n",
    "sets = []\n",
    "\n",
    "for y in word_to_int:\n",
    "    empty = []\n",
    "    if y in index_2:\n",
    "        for i in range(len(word_to_int)):\n",
    "            empty.append(index_2[y][i][0])\n",
    "\n",
    "    sets.append(set(empty))        \n",
    "\n",
    "sets = [*set.intersection(*sets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I have the articles I need the following to perform the dot product:\n",
    "- A\n",
    "- B \n",
    "- ||A||\n",
    "- ||B||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_2\n",
      "0.1657716999054231\n",
      "article_4\n",
      "0.14185478949654853\n"
     ]
    }
   ],
   "source": [
    "# the logic is as followed. For each article found. We get the its vector of tdidf\n",
    "# then we uset\n",
    "\n",
    "\n",
    "for x in sets:\n",
    "    print(x)\n",
    "    # finfin starts at 0 but article at 1\n",
    "    x = x.replace(\"article_\",\"\")\n",
    "    # get we alll the scores and transform into a vector\n",
    "    A = [q[1] for q in finfin[int(x)-1]]\n",
    "    A = np.array(A)\n",
    "\n",
    "    len_a_tfidf = np.linalg.norm(A)\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    # For each word in the query, I want to replace the zero of the np.zeros vector \n",
    "    # Now, to replace the right 0, I need\n",
    "    \n",
    "    # Now it must be noted, it would be easier maybe to copy the vector A. and just replace by 0 for the some index.\n",
    "    \n",
    "    for i in word_to_int:\n",
    "    \n",
    "        A_index = [k[0] for k in finfin[int(x)-1]]\n",
    "        the_index = A_index.index(i)\n",
    "        indexes.append(finfin[int(x)-1][the_index])\n",
    "\n",
    "    B = np.zeros(len(A))\n",
    "    \n",
    "    for j in indexes:\n",
    "        B[j[0]] = j[1]\n",
    "\n",
    "    len_b_tfidf = np.linalg.norm(B)\n",
    "\n",
    "    Cossim = np.dot(A,B)/(len_a_tfidf*len_b_tfidf)\n",
    "    print(Cossim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cannot use this library\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "list_plots = []\n",
    "for x in range(1,5):\n",
    "    with open(\"article_\"+str(x)+\".tsv\", \"r\") as article:\n",
    "        data = pd.read_csv(article, sep = \"\\t\", encoding = \"utf-8\")\n",
    "        list_plots.append(data[\"Plot\"].item())\n",
    "        \n",
    "tokenized_plots = [word_tokenize(doc) for doc in list_plots]\n",
    "\n",
    "dictio = Dictionary(tokenized_plots)\n",
    "\n",
    "corpus = [dictio.doc2bow(doc) for doc in tokenized_plots]\n",
    "\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "tfidf[corpus[0]]\n",
    "\n",
    "# Now take the first example -> the book \"Jack\"\n",
    "# because we know from before that NO word in this plot will appear in the other plots all weights should be equal\n",
    "#print(tfidf[corpus[1]])\n",
    "#print(\"\\n\")\n",
    "# to acess individual elements\n",
    "#print(tfidf[corpus[1]][0])\n",
    "#print(\"\\n\")\n",
    "# To access the value of the tdidf score of the word\n",
    "#print(tfidf[corpus[0]][0][1])\n",
    "\n",
    "# We can use this to get a dictionary -> we could do this in the first part of this question\n",
    "dictionary = dictio.token2id # we have to keep this indexing \n",
    "\n",
    "# now i have to build the invertex index again\n",
    "\n",
    "index_2 = {}\n",
    "\n",
    "for k,v in dictionary.items():\n",
    "    for x in range(1,5):\n",
    "        with open(\"article_\"+str(x)+\".tsv\", \"r\") as article:\n",
    "            data = pd.read_csv(article, sep = \"\\t\", encoding = \"utf-8\")\n",
    "            if k in ([x for x in data[\"Plot\"].item().split()]):\n",
    "                if v not in index_2:\n",
    "                    # Now i dont fucking get why we need these fucking integers when words are perfectly fine\n",
    "                    # [x-1] -> because the corpus starts at 0\n",
    "                    # [v] -> because we want the v th element of the \n",
    "                    index_2[v] = [(\"article_\"+str(x), tfidf[corpus[x-1]][[y[0] for y in tfidf[corpus[x-1]]].index(v)][1])]\n",
    "                elif v in index_2:\n",
    "                    index_2[v].append((\"article_\"+str(x), tfidf[corpus[x-1]][[y[0] for y in tfidf[corpus[x-1]]].index(v)][1]))\n",
    "\n",
    "dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
