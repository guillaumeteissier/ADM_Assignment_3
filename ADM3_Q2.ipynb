{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from lxml import html\n",
    "import requests\n",
    "from langdetect import detect\n",
    "import math\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_plot(plot):\n",
    "    \"\"\" plot -> string type\"\"\"\n",
    "    \"\"\" return processed plot -> string \"\"\"\n",
    "    \n",
    "    #tokenisation\n",
    "    word_tokens = word_tokenize(plot) \n",
    "    # stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filter_stopsword = [w for w in word_tokens if not w in stop_words]\n",
    "    # punctuations\n",
    "    filter_punc = [w for w in filter_stopsword if w.isalnum()]\n",
    "    # stemming\n",
    "    ps = PorterStemmer() \n",
    "    filter_stem = [ps.stem(w) for w in filter_punc]\n",
    "    # single string\n",
    "    clean_plot = \" \".join(filter_stem)\n",
    "    \n",
    "    return clean_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the preprocessing and the creation of the vocabulary in one go\n",
    "word_list = []\n",
    "\n",
    "# start looping over the tsv files\n",
    "n = 26885+1 # the total nb of articles we have\n",
    "for x in range(1,n):\n",
    "    # article_\n",
    "    data = pd.read_csv(\"html_tsv_test_\"+str(x)+\".tsv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "    plot = data['Plot'].item()\n",
    "    \n",
    "    # replace the old plot with the new one\n",
    "    data[\"Plot\"] = data[\"Plot\"].replace(data['Plot'].item(),clean_plot(plot))\n",
    "    \n",
    "    # ideally we will save over the old file\n",
    "    data.to_csv(\"article_\"+str(x)+\".tsv\", sep = '\\t')\n",
    "    \n",
    "    # THIS PART TO CREATE THE VOCABULARY\n",
    "    word_list.append(clean_plot(plot).split())\n",
    "\n",
    "# the Vocabulary is a list of list, we flatten\n",
    "flat_vocabulary = [item for sublist in word_list for item in sublist]\n",
    "# putting the elements in a dico will eliminate the duplicates\n",
    "vocabulary = dict.fromkeys(flat_vocabulary)\n",
    "\n",
    "# We get the \"integers\" we are asked\n",
    "i = 0\n",
    "for k,v in vocabulary.items():\n",
    "    vocabulary[k] = i\n",
    "    i = i +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Guillaume/Desktop/ADM_HM3\"\n",
    "os.chdir(path)\n",
    "with open(\"vocabulary.json\",\"w\",encoding=\"utf-8\") as outfile:\n",
    "    outfile.write(json.dumps(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Conjunctive Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Create your index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(\"vocabulary.json\", \"r\")\n",
    "vocabulary = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVS make it faster than opening and closing each time\n",
    "# changed to the correct CSV file \n",
    "df = pd.read_csv(r\"articles_title_plot_url.csv\", nrows=10000)\n",
    "\n",
    "df[\"Tokens\"] = df['Plot'].apply(lambda x: word_tokenize(clean_plot(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_voca={}\n",
    "# We create a new vocabulary but without the integers and instead replace by a list we will fill\n",
    "# We end up with something \"could : []\"\n",
    "# We need to replace could by an int and fill the list with the articles\n",
    "for k,v in vocabulary.items():\n",
    "    empty_voca.update({k:[]})\n",
    "\n",
    "# We use the index generated by the enumerate function\n",
    "# index + 1 = article number\n",
    "# So we have something like: \"could : [1,25, 25,100,....]\"\n",
    "# We could get rid of the duplicates, they are the words that appear multiple times in a plot -> but not now\n",
    "# We need to replace \"could\" with an integer. -> we will use the one that were in the vocabulary\n",
    "for index, tokens in enumerate(df[\"Tokens\"]):\n",
    "    for token in tokens:\n",
    "        empty_voca[token].append(\"article_\"+str(index+1))\n",
    "\n",
    "values = [x for x in empty_voca.values()]\n",
    "index_1 = {v: k for k, v in vocabulary.items()}\n",
    "\n",
    "i = 0\n",
    "for k,v in index_1.items():\n",
    "    index_1.update({k:values[i]})\n",
    "    i += 1\n",
    "index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_input_1(query):\n",
    "    \n",
    "    # we clean the input\n",
    "    query = clean_plot(query)\n",
    "    # we split each word of the query\n",
    "    query_words = query.split()\n",
    "\n",
    "    # We map the word of the query to their integers.\n",
    "    word_to_int = []\n",
    "    for word in query_words:\n",
    "        # will have to check if that \"in\" works as intended\n",
    "        if word in vocabulary:\n",
    "            word_to_int.append(vocabulary[word])\n",
    "\n",
    "    # We ONLY want the articles for which ALL query words are present in the plot\n",
    "    sets = []\n",
    "    for y in word_to_int:\n",
    "        # Does 12 in 120 return true ???\n",
    "        if y in index_1:\n",
    "            sets.append(set(index_1[y]))\n",
    "\n",
    "    # We look for the intersection of articles\n",
    "    sets = [*set.intersection(*sets)]\n",
    "    \n",
    "    return sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(articles_set):\n",
    "    \n",
    "    df = pd.DataFrame(columns = [\"bookTitle\",\"Plot\",\"URL\"])\n",
    "    \n",
    "    path = \"C:/Users/Guillaume/Desktop/ADM_HM3/fuck_no_empty/\"\n",
    "    \n",
    "    for article in articles_set:\n",
    "        \n",
    "        article = open(path+article+\".tsv\", \"r\")\n",
    "        data = pd.read_csv(article, sep = \"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "        title = data[\"bookTitle\"].iloc[0]\n",
    "        plot = data[\"Plot\"].iloc[0]\n",
    "        url = data[\"URL\"].iloc[0]\n",
    "\n",
    "        to_append = [title, plot, url]\n",
    "        df_length = len(df)\n",
    "        df.loc[df_length] = to_append\n",
    "    \n",
    "    return df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input()\n",
    "get_output(query_input_1(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVS make it faster than opening and closing each time\n",
    "df = pd.read_csv(r\"articles_title_plot_url.csv\", nrows=1000)\n",
    "\n",
    "df[\"Tokens\"] = df['Plot'].apply(lambda x: word_tokenize(clean_plot(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_3 = index_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dfi is the count of documents where the term i is present\n",
    "new_ = {k:len(set(v)) for k,v in index_3.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfi is the count of term i in document j\n",
    "# we can just use a counter functions\n",
    "df[\"Token_Counter\"] = df[\"Tokens\"].apply(lambda x : Counter(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finfin = []\n",
    "\n",
    "for index,tokens in enumerate(df[\"Token_Counter\"]):\n",
    "    for k,v in tokens.items():\n",
    "        # v is the term frequency of term i in document j\n",
    "        # len(df) is the count of documents\n",
    "        # new_[vocabulary[k]] is first: vocabulary[k] \n",
    "        finfin.append((index, vocabulary[k], (v) * math.log(len(df)/new_[vocabulary[k]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2 = {}\n",
    "\n",
    "for k,v in vocabulary.items():\n",
    "    index_2.update({v:[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in index_2.items():\n",
    "    for x in finfin:\n",
    "        if x[1] == k:\n",
    "            v.append((x[0]+1,x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_int(query):\n",
    "    \n",
    "    query = clean_plot(query)\n",
    "    split_input = query.split()\n",
    "\n",
    "    word_to_int = []\n",
    "    for word in split_input:\n",
    "        if word in vocabulary:\n",
    "            word_to_int.a ppend(vocabulary[word])\n",
    "    \n",
    "    return word_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_input_2(query):\n",
    "\n",
    "    sets = []\n",
    "    for y in word_to_int(query):\n",
    "        if y in index_2:\n",
    "            sets.append(index_2[y])\n",
    "\n",
    "    new_sets = []\n",
    "    for x in sets:\n",
    "        list_articles = []\n",
    "        for y in x:\n",
    "            list_articles.append(y[0])\n",
    "        new_sets.append(set(list_articles))\n",
    "\n",
    "    sets = [*set.intersection(*new_sets)]\n",
    "\n",
    "    return sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(article, query):\n",
    "    \n",
    "    A = [x[2] for x in finfin if x[0] == article-1]\n",
    "    A_int_words = [x[1] for x in finfin if x[0] == article-1]\n",
    "\n",
    "    indexes = []\n",
    "    for i in word_to_int(query):\n",
    "        A_index = A_int_words.index(i)\n",
    "        indexes.append(A_index)\n",
    "\n",
    "    B = np.zeros(len(A))\n",
    "\n",
    "    for j in indexes:\n",
    "        B[j] = A[j]\n",
    "\n",
    "    A = np.array(A)\n",
    "    len_a_tfidf = np.linalg.norm(A)\n",
    "    len_b_tfidf = np.linalg.norm(B)\n",
    "\n",
    "    cossim = np.dot(A,B)/(len_a_tfidf*len_b_tfidf)\n",
    "    \n",
    "    return cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_score(articles_set, query):\n",
    "    \n",
    "    df = pd.DataFrame(columns = [\"bookTitle\",\"Plot\",\"URL\",\"Similarity\"])\n",
    "\n",
    "    path = \"C:/Users/Guillaume/Desktop/ADM_HM3/fuck_no_empty/\"\n",
    "    \n",
    "    for article in articles_set:\n",
    "        \n",
    "        article_doc = open(path+\"article_\"+str(article)+\".tsv\", \"r\", encoding=\"utf-8\")\n",
    "        data = pd.read_csv(article_doc, sep = \"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "        title = data[\"bookTitle\"].iloc[0]\n",
    "        plot = data[\"Plot\"].iloc[0]\n",
    "        url = data[\"URL\"].iloc[0]\n",
    "        Sim_score = float(cossim(article, query))\n",
    "\n",
    "        to_append = [title, plot, url, Sim_score]\n",
    "        df_length = len(df)\n",
    "        df.loc[df_length] = to_append\n",
    "    \n",
    "    return df.drop_duplicates().sort_values(by=\"Similarity\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hitler\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>URL</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Power of One</td>\n",
       "      <td>In 1939, as Hitler casts his enormous, cruel s...</td>\n",
       "      <td>https://www.goodreads.com/book/show/122.The_Po...</td>\n",
       "      <td>0.188018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Complete Maus</td>\n",
       "      <td>Combined for the first time here are Maus I: A...</td>\n",
       "      <td>https://www.goodreads.com/book/show/15195.The_...</td>\n",
       "      <td>0.171166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Bronze Horseman</td>\n",
       "      <td>The golden skies, the translucent twilight, th...</td>\n",
       "      <td>https://www.goodreads.com/book/show/41014505-t...</td>\n",
       "      <td>0.116016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seabiscuit: An American Legend</td>\n",
       "      <td>There's an alternate cover edition hereSeabisc...</td>\n",
       "      <td>https://www.goodreads.com/book/show/110737.Sea...</td>\n",
       "      <td>0.092499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Corelli's Mandolin</td>\n",
       "      <td>Captain Corelli’s Mandolin is set in the early...</td>\n",
       "      <td>https://www.goodreads.com/book/show/3388.Corel...</td>\n",
       "      <td>0.069524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        bookTitle  \\\n",
       "2                The Power of One   \n",
       "1               The Complete Maus   \n",
       "3             The Bronze Horseman   \n",
       "0  Seabiscuit: An American Legend   \n",
       "4              Corelli's Mandolin   \n",
       "\n",
       "                                                Plot  \\\n",
       "2  In 1939, as Hitler casts his enormous, cruel s...   \n",
       "1  Combined for the first time here are Maus I: A...   \n",
       "3  The golden skies, the translucent twilight, th...   \n",
       "0  There's an alternate cover edition hereSeabisc...   \n",
       "4  Captain Corelli’s Mandolin is set in the early...   \n",
       "\n",
       "                                                 URL  Similarity  \n",
       "2  https://www.goodreads.com/book/show/122.The_Po...    0.188018  \n",
       "1  https://www.goodreads.com/book/show/15195.The_...    0.171166  \n",
       "3  https://www.goodreads.com/book/show/41014505-t...    0.116016  \n",
       "0  https://www.goodreads.com/book/show/110737.Sea...    0.092499  \n",
       "4  https://www.goodreads.com/book/show/3388.Corel...    0.069524  "
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input()\n",
    "get_output_score(query_input_2(query), query).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
