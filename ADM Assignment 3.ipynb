{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM Homework #3 - Which book would you recommend ?\n",
    "\n",
    "Lorenzo Garcia -\n",
    "Guillaume Teissier -\n",
    "Th√©ophile Tolani\n",
    "\n",
    "### Goal of the homework: \n",
    "Build a search engine over the \"best books ever\" list of GoodReads. Unless differently specified, all the functions must be implemented from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "For this homework, there is no provided dataset, but you have to build your own. Your search engine will run on text documents. So, here we detail the procedure to follow for the data collection.\n",
    "\n",
    "### 1.1. Get the list of books\n",
    "We start from the list of books to include in your corpus of documents. In particular, we focus on the best books ever list. From this list we want to collect the url associated to each book in the list. As you realize, the list is long and splitted in many pages. We ask you to retrieve only the urls of the books listed in the first 300 pages.\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getURL(page):\n",
    "    \"\"\"\n",
    "    :param page: html of web page (here: Python home page) \n",
    "    :return: urls in that page \n",
    "    \"\"\"\n",
    "    start_link = page.find(\"a href\")\n",
    "    if start_link == -1:\n",
    "        return None, 0\n",
    "    start_quote = page.find('\"', start_link)\n",
    "    end_quote = page.find('\"', start_quote + 1)\n",
    "    url = page[start_quote + 1: end_quote]\n",
    "    return url, end_quote\n",
    "\n",
    "def PageScraper(url):\n",
    "    l=[]\n",
    "    response = requests.get(url)\n",
    "    page = str(BeautifulSoup(response.content))\n",
    "    while url!= '#comment_form':\n",
    "        url, n = getURL(page)\n",
    "        page = page[n:]\n",
    "        if url and url[:5]=='/book':\n",
    "            l.append(url)\n",
    "    return l\n",
    "\n",
    "#Scraps the first 100 pages\n",
    "outfile = open(\"test.txt\", \"a\")\n",
    "for k in range(1,101):\n",
    "    url = 'https://www.goodreads.com/list/show/1.Best_Books_Ever?page='+str(k)\n",
    "    L = PageScraper(url)\n",
    "    for k in range(len(L)):\n",
    "        outfile.write(L[k])\n",
    "        outfile.write(\"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl books\n",
    "Once you get all the urls in the first 300 pages of the list, you:\n",
    "\n",
    "Download the html corresponding to each of the collected urls.\n",
    "After you collect a single page, immediatly save its html in a file. In this way, if your program stops, for any reason, you will not loose the data collected up to the stopping point. More details in Important (2).\n",
    "Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the books in page 1, page 2, ... of the list of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
