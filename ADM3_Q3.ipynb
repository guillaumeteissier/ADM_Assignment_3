{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import lxml\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from lxml import html\n",
    "import requests\n",
    "from langdetect import detect\n",
    "import math\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_plot(plot):\n",
    "    \"\"\" plot -> string type\"\"\"\n",
    "    \"\"\" return processed plot -> string \"\"\"\n",
    "    \n",
    "    #tokenisation\n",
    "    word_tokens = word_tokenize(plot) \n",
    "    # stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filter_stopsword = [w for w in word_tokens if not w in stop_words]\n",
    "    # punctuations\n",
    "    filter_punc = [w for w in filter_stopsword if w.isalnum()]\n",
    "    # stemming\n",
    "    ps = PorterStemmer() \n",
    "    filter_stem = [ps.stem(w) for w in filter_punc]\n",
    "    # single string\n",
    "    clean_plot = \" \".join(filter_stem)\n",
    "    \n",
    "    return clean_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the preprocessing and the creation of the vocabulary in one go\n",
    "word_list = []\n",
    "\n",
    "# start looping over the tsv files\n",
    "n = 26885+1 # the total nb of articles we have\n",
    "for x in range(1,n):\n",
    "    # article_\n",
    "    data = pd.read_csv(\"html_tsv_test_\"+str(x)+\".tsv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "    plot = data['Plot'].item()\n",
    "    \n",
    "    # replace the old plot with the new one\n",
    "    data[\"Plot\"] = data[\"Plot\"].replace(data['Plot'].item(),clean_plot(plot))\n",
    "    \n",
    "    # ideally we will save over the old file\n",
    "    data.to_csv(\"article_\"+str(x)+\".tsv\", sep = '\\t')\n",
    "    \n",
    "    # THIS PART TO CREATE THE VOCABULARY\n",
    "    word_list.append(clean_plot(plot).split())\n",
    "\n",
    "# the Vocabulary is a list of list, we flatten\n",
    "flat_vocabulary = [item for sublist in word_list for item in sublist]\n",
    "# putting the elements in a dico will eliminate the duplicates\n",
    "vocabulary = dict.fromkeys(flat_vocabulary)\n",
    "\n",
    "# We get the \"integers\" we are asked\n",
    "i = 0\n",
    "for k,v in vocabulary.items():\n",
    "    vocabulary[k] = i\n",
    "    i = i +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Guillaume/Desktop/ADM_HM3\"\n",
    "os.chdir(path)\n",
    "with open(\"vocabulary.json\",\"w\",encoding=\"utf-8\") as outfile:\n",
    "    outfile.write(json.dumps(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(\"vocabulary.json\", \"r\")\n",
    "vocabulary = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVS make it faster than opening and closing each time\n",
    "df = pd.read_csv(r\"output.csv\", nrows=1000)\n",
    "\n",
    "df[\"Tokens\"] = df['Plot'].apply(lambda x: word_tokenize(clean_plot(x)))\n",
    "df[\"Date\"] = df[\"Date\"].apply(lambda x: str(x).split()[-1])\n",
    "df[\"Char_Tokens\"] = df[\"Setting\"].apply(lambda x: word_tokenize(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_voca={}\n",
    "\n",
    "for k,v in vocabulary.items():\n",
    "    empty_voca.update({k:[]})\n",
    "\n",
    "for index, tokens in enumerate(df[\"Tokens\"]):\n",
    "    for token in tokens:\n",
    "        empty_voca[token].append(\"article_\"+str(index+1))\n",
    "\n",
    "values = [x for x in empty_voca.values()]\n",
    "index_1 = {v: k for k, v in vocabulary.items()}\n",
    "\n",
    "i = 0\n",
    "for k,v in index_1.items():\n",
    "    index_1.update({k:values[i]})\n",
    "    i += 1\n",
    "index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_input_1(query):\n",
    "    \n",
    "    # PLOT PART\n",
    "    # we clean the input\n",
    "    query_words = clean_plot(query)\n",
    "    query_words = query_words.split()\n",
    "    \n",
    "    word_to_int = []\n",
    "    for word in query_words:\n",
    "        if word in vocabulary:\n",
    "            word_to_int.append(vocabulary[word])\n",
    "    \n",
    "    sets = []\n",
    "    for y in word_to_int:\n",
    "        if y in index_1:\n",
    "            sets.append(set(index_1[y]))\n",
    "    \n",
    "    sets = [*set.intersection(*sets)]\n",
    "    \n",
    "    return sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nssf(article, q_year, q_char, q_rating):\n",
    "    \n",
    "    \"\"\" 1+(Nb common characters)  * Exp ^ + 1 / (year asked â€“ year actual book)) + 1/(rating count - actual rating) \"\"\"\n",
    "    \n",
    "    path = \"C:/Users/Guillaume/Desktop/ADM_HM3/fuck_no_empty/\"\n",
    "    article = open(path+article+\".tsv\", \"r\")\n",
    "    \n",
    "    data = pd.read_csv(article, sep = \"\\t\", encoding = \"utf-8\")\n",
    "    \n",
    "    data[\"Char_Tokens\"] = data[\"Setting\"].apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "    q_char = set([x.lower() for x in q_char])\n",
    "    com_char = 0\n",
    "    for index,tokens in enumerate(data[\"Char_Tokens\"]):\n",
    "        intersect = q_char.intersection(set([y.lower() for y in tokens]))\n",
    "        com_char = len(intersect)\n",
    "            \n",
    "    data[\"Date\"] = data[\"Date\"].apply(lambda x: str(x).split()[-1])\n",
    "    year_diff = abs(int(data[\"Date\"].iloc[0]) - int(q_year))\n",
    "    \n",
    "    rating_diff = abs(float(data[\"ratingValue\"].iloc[0]) - int(q_rating))\n",
    "        \n",
    "    new_score = 1 + com_char * math.exp((1/year_diff)+(1/(rating_diff)))\n",
    "    \n",
    "    return new_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(articles_set, q_year, q_char, q_rating):\n",
    "    \n",
    "    df = pd.DataFrame(columns = [\"bookTitle\",\"Plot\",\"URL\",\"Similarity\"])\n",
    "    \n",
    "    path = \"C:/Users/Guillaume/Desktop/ADM_HM3/fuck_no_empty/\"\n",
    "    \n",
    "    for article in articles_set:\n",
    "        \n",
    "        article_doc = open(path+article+\".tsv\", \"r\",encoding=\"utf-8\")\n",
    "        data = pd.read_csv(article_doc, sep = \"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "        title = data[\"bookTitle\"].iloc[0]\n",
    "        plot = data[\"Plot\"].iloc[0]\n",
    "        url = data[\"URL\"].iloc[0]\n",
    "        similarity = nssf(article, q_year, q_char, q_rating)\n",
    "\n",
    "        to_append = [title, plot, url, similarity]\n",
    "        df_length = len(df)\n",
    "        df.loc[df_length] = to_append\n",
    "    \n",
    "    return df.drop_duplicates().sort_values(by=\"Similarity\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>URL</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Harry Potter and the Chamber of Secrets</td>\n",
       "      <td>Ever since Harry Potter had come home for the ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/15881.Harr...</td>\n",
       "      <td>28.815166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter and the Goblet of Fire</td>\n",
       "      <td>Harry Potter is midway through his training as...</td>\n",
       "      <td>https://www.goodreads.com/book/show/6.Harry_Po...</td>\n",
       "      <td>20.665414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
       "      <td>For twelve long years, the dread fortress of A...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5.Harry_Po...</td>\n",
       "      <td>15.843071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Sorcerer's Stone</td>\n",
       "      <td>Harry Potter's life is miserable. His parents ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/3.Harry_Po...</td>\n",
       "      <td>12.716439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Harry Potter is leaving Privet Drive for the l...</td>\n",
       "      <td>https://www.goodreads.com/book/show/136251.Har...</td>\n",
       "      <td>12.575694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harry Potter and the Half-Blood Prince</td>\n",
       "      <td>The war against Voldemort is not going well; e...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1.Harry_Po...</td>\n",
       "      <td>7.828142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Harry Potter Series Box Set</td>\n",
       "      <td>Over 4000 pages of Harry Potter and his world,...</td>\n",
       "      <td>https://www.goodreads.com/book/show/862041.Har...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   bookTitle  \\\n",
       "6    Harry Potter and the Chamber of Secrets   \n",
       "0        Harry Potter and the Goblet of Fire   \n",
       "4   Harry Potter and the Prisoner of Azkaban   \n",
       "3      Harry Potter and the Sorcerer's Stone   \n",
       "2       Harry Potter and the Deathly Hallows   \n",
       "5     Harry Potter and the Half-Blood Prince   \n",
       "10               Harry Potter Series Box Set   \n",
       "\n",
       "                                                 Plot  \\\n",
       "6   Ever since Harry Potter had come home for the ...   \n",
       "0   Harry Potter is midway through his training as...   \n",
       "4   For twelve long years, the dread fortress of A...   \n",
       "3   Harry Potter's life is miserable. His parents ...   \n",
       "2   Harry Potter is leaving Privet Drive for the l...   \n",
       "5   The war against Voldemort is not going well; e...   \n",
       "10  Over 4000 pages of Harry Potter and his world,...   \n",
       "\n",
       "                                                  URL  Similarity  \n",
       "6   https://www.goodreads.com/book/show/15881.Harr...   28.815166  \n",
       "0   https://www.goodreads.com/book/show/6.Harry_Po...   20.665414  \n",
       "4   https://www.goodreads.com/book/show/5.Harry_Po...   15.843071  \n",
       "3   https://www.goodreads.com/book/show/3.Harry_Po...   12.716439  \n",
       "2   https://www.goodreads.com/book/show/136251.Har...   12.575694  \n",
       "5   https://www.goodreads.com/book/show/1.Harry_Po...    7.828142  \n",
       "10  https://www.goodreads.com/book/show/862041.Har...    1.000000  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"potter\"\n",
    "q_year = 2000\n",
    "q_char = [\"Sirius\", \"Harry\"]\n",
    "q_rating = 4.2\n",
    "get_output(query_input_1(query) ,q_year, q_char, q_rating)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
